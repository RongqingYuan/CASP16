{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.chdir(\"/home2/s439906/project/CASP16/oligomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_target(results_dir, v1_file, v2_file, out_dir,\n",
    "                    feature, model, mode, impute_value):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    data = pd.DataFrame()\n",
    "    data_weighted = pd.DataFrame()\n",
    "    data_raw = pd.DataFrame()\n",
    "    target_v1_model_v1 = None\n",
    "    target_v1_model_v2 = None\n",
    "    target_v2_model_v1 = None\n",
    "    target_v2_model_v2 = None\n",
    "    template_raw_file = out_dir + \\\n",
    "        f\"{feature}-{model}-{mode}-impute={impute_value}.csv\"\n",
    "    template_EU_file = out_dir + \\\n",
    "        f\"{feature}-{model}-{mode}-impute={impute_value}_weighted_EU.csv\"\n",
    "    for file in [v1_file, v2_file]:\n",
    "        # the nomenclature is bad here, ignore it because it is just a small script.\n",
    "        v1_path = results_dir + file\n",
    "        v1_data = pd.read_csv(v1_path, sep=\"\\t\", index_col=0)\n",
    "        ############################\n",
    "        nr_interf_group_in_ref = v1_data[\"nr_interf_group_in_ref\"]\n",
    "        if nr_interf_group_in_ref.nunique() == 1:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Not all values are the same in nr_interf_group_in_ref\")\n",
    "        nr_refinterf_num = v1_data[\"nr_refinterf_#\"]\n",
    "        if nr_refinterf_num.nunique() == 1:\n",
    "            nr_refinterf_num = nr_refinterf_num.astype(str)\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Not all values are the same in nr_refinterf_#\")\n",
    "        nr_interf_group_interfsize_in_ref = v1_data[\"nr_interf_group_interfsize_in_ref\"]\n",
    "        if nr_interf_group_interfsize_in_ref.nunique() == 1:\n",
    "            nr_interf_group_interfsize_in_ref = nr_interf_group_interfsize_in_ref.astype(\n",
    "                str)\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Not all values are the same in nr_interf_group_interfsize_in_ref\")\n",
    "        ############################\n",
    "        v1_data = pd.DataFrame(v1_data[feature]).astype(str)\n",
    "        v1_data_split = v1_data[feature].str.split(';', expand=True)\n",
    "        v1_data_split.columns = [\n",
    "            f'interface_{i+1}' for i in range(v1_data_split.shape[1])]\n",
    "        # fill \"-\" with nan\n",
    "        v1_data_split = v1_data_split.replace(\"-\", np.nan)\n",
    "        v1_data_split = v1_data_split.astype(float)\n",
    "        v1_data_split.index = v1_data_split.index.str.extract(\n",
    "            r'(\\w+)TS(\\w+)_(\\w+)').apply(lambda x: (f\"{x[0]}\", f\"TS{x[1]}\", x[2][0]), axis=1)\n",
    "        v1_data_split.index = pd.MultiIndex.from_tuples(\n",
    "            v1_data_split.index, names=['target', 'group', 'submission_id'])\n",
    "        if model == \"best\":\n",
    "            v1_data_split = v1_data_split.loc[(slice(None), slice(None), [\n",
    "                \"1\", \"2\", \"3\", \"4\", \"5\"]), :]\n",
    "        elif model == \"first\":\n",
    "            v1_data_split = v1_data_split.loc[(slice(None), slice(None),\n",
    "                                               \"1\"), :]\n",
    "        elif model == \"sixth\":\n",
    "            v1_data_split = v1_data_split.loc[(slice(None), slice(None),\n",
    "                                               \"6\"), :]\n",
    "\n",
    "        if file == v1_file:\n",
    "            for i in range(v1_data_split.shape[1]):\n",
    "                grouped = v1_data_split.groupby([\"target\", \"group\"])\n",
    "                feature_name = f\"interface_{i+1}\"\n",
    "                grouped = pd.DataFrame(grouped[feature_name].max())\n",
    "                print(grouped)\n",
    "                target_v1_model_v1 = grouped.loc[\"T1249v1\", :]\n",
    "                # add _v1 to the column name of target_v1_model_v1\n",
    "                target_v1_model_v1 = target_v1_model_v1.rename(\n",
    "                    columns={feature_name: f\"{v1_file.split('.')[0]}_{feature_name}\"})\n",
    "                print(target_v1_model_v1)\n",
    "                target_v1_model_v2 = grouped.loc[\"T1249v2\", :]\n",
    "                # add _v2 to the column name of target_v1_model_v2\n",
    "                target_v1_model_v2 = target_v1_model_v2.rename(\n",
    "                    columns={feature_name: f\"{v2_file.split('.')[0]}_{feature_name}\"})\n",
    "                print(target_v1_model_v2)\n",
    "        elif file == v2_file:\n",
    "            for i in range(v1_data_split.shape[1]):\n",
    "                grouped = v1_data_split.groupby([\"target\", \"group\"])\n",
    "                feature_name = f\"interface_{i+1}\"\n",
    "                grouped = pd.DataFrame(grouped[feature_name].max())\n",
    "                target_v2_model_v1 = grouped.loc[\"T1249v1\", :]\n",
    "                # add _v1 to the column name of target_v2_model_v1\n",
    "                target_v2_model_v1 = target_v2_model_v1.rename(\n",
    "                    columns={feature_name: f\"{v1_file.split('.')[0]}_{feature_name}\"})\n",
    "                print(target_v2_model_v1)\n",
    "                target_v2_model_v2 = grouped.loc[\"T1249v2\", :]\n",
    "                # add _v2 to the column name of target_v2_model_v2\n",
    "                target_v2_model_v2 = target_v2_model_v2.rename(\n",
    "                    columns={feature_name: f\"{v2_file.split('.')[0]}_{feature_name}\"})\n",
    "                print(target_v2_model_v2)\n",
    "\n",
    "    case_1 = pd.concat([target_v1_model_v1, target_v2_model_v2], axis=1)\n",
    "    case_2 = pd.concat([target_v1_model_v2, target_v2_model_v1], axis=1)\n",
    "    # this is a combinatorial problem, we need to choose the best one with restrictions\n",
    "    # fortunately there are only 2 combinations...\n",
    "    case_1[\"sum\"] = case_1.sum(axis=1)\n",
    "    case_2[\"sum\"] = case_2.sum(axis=1)\n",
    "    # choose the best one for each group\n",
    "    best_df = pd.DataFrame()\n",
    "    for group in case_1.index:\n",
    "        if case_1.loc[group, \"sum\"] > case_2.loc[group, \"sum\"]:\n",
    "            best_df = pd.concat([best_df, case_1.loc[group, :].to_frame().T])\n",
    "        else:\n",
    "            best_df = pd.concat([best_df, case_2.loc[group, :].to_frame().T])\n",
    "    best_df = best_df.drop(columns=[\"sum\"])\n",
    "    best_df = best_df.rename(columns={f\"{feature}_v1\": v1_file.split(\".\")[\n",
    "        0], f\"{feature}_v2\": v2_file.split(\".\")[0]})\n",
    "    # sort the best_df by alphabetical order\n",
    "    best_df = best_df.reindex(sorted(best_df.columns), axis=1)\n",
    "    best_df = best_df.sort_index()\n",
    "    template_raw_df = pd.read_csv(template_raw_file, index_col=0)\n",
    "    print(best_df)\n",
    "    print(template_raw_df)\n",
    "    template_raw_df = pd.concat([template_raw_df, best_df], axis=1)\n",
    "    # # sort the template_raw_df by alphabetical order\n",
    "    # template_raw_df = template_raw_df.reindex(sorted(template_raw_df.columns), axis=1)\n",
    "    # template_raw_df = template_raw_df.sort_index()\n",
    "    # template_raw_df.to_csv(template_raw_file)\n",
    "\n",
    "    # grouped = grouped.sort_values(by=feature_name, ascending=False)\n",
    "    # initial_z = (grouped - grouped.mean()) / grouped.std()\n",
    "    # new_z_score = pd.DataFrame(\n",
    "    #     index=grouped.index, columns=grouped.columns)\n",
    "    # filtered_data = grouped[feature_name][initial_z[feature_name] >= -2]\n",
    "    # new_mean = filtered_data.mean(skipna=True)\n",
    "    # new_std = filtered_data.std(skipna=True)\n",
    "    # new_z_score[feature_name] = (\n",
    "    #     grouped[feature_name] - new_mean) / new_std\n",
    "    # new_z_score = new_z_score.fillna(impute_value)\n",
    "    # new_z_score = new_z_score.where(\n",
    "    #     new_z_score > impute_value, impute_value)\n",
    "    # new_z_score = new_z_score.rename(\n",
    "    #     columns={feature_name: v1_file.split(\".\")[0]+\"_\"+feature_name})\n",
    "    # data = pd.concat([data, new_z_score], axis=1)\n",
    "    # grouped = grouped.rename(\n",
    "    #     columns={feature_name: v1_file.split(\".\")[0]+\"_\"+feature_name})\n",
    "    # data_raw = pd.concat([data_raw, grouped], axis=1)\n",
    "    template_EU_df = pd.read_csv(template_EU_file, index_col=0)\n",
    "    # drop sum column\n",
    "    template_EU_df = template_EU_df.drop(columns=[\"sum\"])\n",
    "    for result_file in [v1_file, v2_file]:\n",
    "        print(\"Processing {}\".format(result_file))\n",
    "        result_path = results_dir + result_file\n",
    "        data_tmp = pd.read_csv(result_path, sep=\"\\t\", index_col=0)\n",
    "\n",
    "        ############################\n",
    "        nr_interf_group_in_ref = data_tmp[\"nr_interf_group_in_ref\"]\n",
    "        if nr_interf_group_in_ref.nunique() == 1:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Not all values are the same in nr_interf_group_in_ref\")\n",
    "        nr_refinterf_num = data_tmp[\"nr_refinterf_#\"]\n",
    "        if nr_refinterf_num.nunique() == 1:\n",
    "            nr_refinterf_num = nr_refinterf_num.astype(str)\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Not all values are the same in nr_refinterf_#\")\n",
    "        nr_interf_group_interfsize_in_ref = data_tmp[\"nr_interf_group_interfsize_in_ref\"]\n",
    "        if nr_interf_group_interfsize_in_ref.nunique() == 1:\n",
    "            nr_interf_group_interfsize_in_ref = nr_interf_group_interfsize_in_ref.astype(\n",
    "                str)\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Not all values are the same in nr_interf_group_interfsize_in_ref\")\n",
    "        ############################\n",
    "\n",
    "        data_tmp = pd.DataFrame(data_tmp[feature]).astype(str)\n",
    "        data_tmp_split = data_tmp[feature].str.split(';', expand=True)\n",
    "        data_tmp_split.columns = [\n",
    "            f'interface_{i+1}' for i in range(data_tmp_split.shape[1])]\n",
    "        data_tmp_split = data_tmp_split.replace(\"-\", np.nan)\n",
    "        data_tmp_split = data_tmp_split.astype(float)\n",
    "\n",
    "        data_tmp_split.index = data_tmp_split.index.str.extract(\n",
    "            r'(\\w+)TS(\\w+)_(\\w+)').apply(lambda x: (f\"{x[0]}\", f\"TS{x[1]}\", x[2][0]), axis=1)\n",
    "        data_tmp_split.index = pd.MultiIndex.from_tuples(\n",
    "            data_tmp_split.index, names=['target', 'group', 'submission_id'])\n",
    "        if model == \"best\":\n",
    "            data_tmp_split = data_tmp_split.loc[(slice(None), slice(None), [\n",
    "                \"1\", \"2\", \"3\", \"4\", \"5\"]), :]\n",
    "        elif model == \"first\":\n",
    "            data_tmp_split = data_tmp_split.loc[(slice(None), slice(None),\n",
    "                                                 \"1\"), :]\n",
    "\n",
    "        nr_refinterf_num = nr_refinterf_num.iloc[0]\n",
    "        nr_interface_weights = nr_refinterf_num.split(\";\")\n",
    "        nr_interface_weights = [float(weight)\n",
    "                                for weight in nr_interface_weights]\n",
    "        nr_interface_weights = np.array(nr_interface_weights)\n",
    "        # nr_interface_weights = nr_interface_weights / nr_interface_weights.sum()\n",
    "\n",
    "        nr_interf_group_interfsize_in_ref = nr_interf_group_interfsize_in_ref.iloc[0]\n",
    "        nr_interf_group_interfsize_in_ref = nr_interf_group_interfsize_in_ref.split(\n",
    "            \";\")\n",
    "\n",
    "        nr_interface_size_weights = []\n",
    "        for i in range(len(nr_interf_group_interfsize_in_ref)):\n",
    "            size_info = nr_interf_group_interfsize_in_ref[i].split(\",\")\n",
    "            size_weight = 0\n",
    "            for j in range(len(size_info)):\n",
    "                sizes = size_info[j].split(\"/\")\n",
    "                for size in sizes:\n",
    "                    size_weight += int(size)\n",
    "            nr_interface_size_weights.append(size_weight/2)\n",
    "        nr_interface_size_weights = np.log10(nr_interface_size_weights)\n",
    "        nr_interface_size_weights = np.array(nr_interface_size_weights)\n",
    "        # nr_interface_size_weights = nr_interface_size_weights / nr_interface_size_weights.sum()\n",
    "\n",
    "        # elementwise multiplication\n",
    "        interface_weight = nr_interface_weights * nr_interface_size_weights\n",
    "        interface_weight = interface_weight / interface_weight.sum()\n",
    "        EU_weight = data_tmp_split.shape[1] ** (1/3)\n",
    "\n",
    "        target = result_file.split(\".\")[0]\n",
    "        data_raw = pd.DataFrame(best_df[f\"{target}_interface_1\"])\n",
    "        # there is only one interface in the best_df so we can just take the first one for simplicity\n",
    "        # get a target_score df that has the same row as data_raw to make sure no nan values\n",
    "        target_score = pd.DataFrame(index=template_EU_df.index)\n",
    "        for i in range(data_raw.shape[1]):\n",
    "            grouped = data_raw\n",
    "            feature_name = f\"{target}_interface_{i+1}\"\n",
    "            initial_z = (grouped - grouped.mean()) / grouped.std()\n",
    "            new_z_score = pd.DataFrame(\n",
    "                index=grouped.index, columns=grouped.columns)\n",
    "            filtered_data = grouped[feature_name][initial_z[feature_name] >= -2]\n",
    "            new_mean = filtered_data.mean(skipna=True)\n",
    "            new_std = filtered_data.std(skipna=True)\n",
    "            new_z_score[feature_name] = (\n",
    "                grouped[feature_name] - new_mean) / new_std\n",
    "            new_z_score = new_z_score.fillna(impute_value)\n",
    "            new_z_score = new_z_score.where(\n",
    "                new_z_score > impute_value, impute_value)\n",
    "            target_score = pd.concat([target_score, new_z_score], axis=1)\n",
    "            target_score = target_score.fillna(impute_value)\n",
    "        print(target_score)\n",
    "        # take the weighted sum of the interface scores\n",
    "        target_score = target_score * interface_weight\n",
    "        data_weighted[result_file.split(\".\")[0]] = target_score.sum(axis=1)\n",
    "        # then multiply by the EU_weight\n",
    "        data_weighted[result_file.split(\".\")[0]] = data_weighted[result_file.split(\n",
    "            \".\")[0]] * EU_weight\n",
    "    print(data_weighted)\n",
    "    template_EU_df = pd.concat([template_EU_df, data_weighted], axis=1)\n",
    "    print(template_EU_df)\n",
    "    # remove rows with any nan values # group 314 only have this target,\n",
    "    # and it will cause nan values in original data when concat\n",
    "    template_raw_df = template_raw_df.dropna(axis=0, how='any')\n",
    "    template_EU_df = template_EU_df.dropna(axis=0, how='any')\n",
    "    # sort the template_raw_df by alphabetical order\n",
    "    template_raw_df = template_raw_df.reindex(\n",
    "        sorted(template_raw_df.columns), axis=1)\n",
    "    template_raw_df = template_raw_df.sort_index()\n",
    "    template_raw_df.to_csv(template_raw_file)\n",
    "\n",
    "    # sort the template_EU_df by alphabetical order\n",
    "    template_EU_df = template_EU_df.reindex(\n",
    "        sorted(template_EU_df.columns), axis=1)\n",
    "    template_EU_df = template_EU_df.sort_index()\n",
    "    # sort the template_EU_df by alphabetical order\n",
    "    template_EU_df[\"sum\"] = template_EU_df.sum(axis=1)\n",
    "    template_EU_df.to_csv(template_EU_file)\n",
    "    template_EU_df.to_csv(\n",
    "        out_dir + f\"{feature}-{model}-{mode}-impute={impute_value}_sum.csv\")\n",
    "    # sys.exit(0)\n",
    "    # data = data.fillna(impute_value)\n",
    "    # data = data.reindex(sorted(data.columns), axis=1)\n",
    "    # data = data.sort_index()\n",
    "    # data_file = f\"{feature}-{model}-{mode}-impute={impute_value}.csv\"\n",
    "    # data.to_csv(out_dir + data_file)\n",
    "\n",
    "    # data[\"sum\"] = data.sum(axis=1)\n",
    "    # data = data.sort_values(by=\"sum\", ascending=False)\n",
    "    # sum_data_file = f\"{feature}-{model}-{mode}-impute={impute_value}_sum.csv\"\n",
    "    # data.to_csv(out_dir + sum_data_file)\n",
    "\n",
    "    # data_raw = data_raw.reindex(sorted(data_raw.columns), axis=1)\n",
    "    # data_raw = data_raw.sort_index()\n",
    "    # data_raw_file = f\"{feature}-{model}-{mode}-impute={impute_value}_raw.csv\"\n",
    "    # data_raw.to_csv(out_dir + data_raw_file)\n",
    "\n",
    "    # data_weighted[\"sum\"] = data_weighted.sum(axis=1)\n",
    "    # data_weighted = data_weighted.reindex(\n",
    "    #     sorted(data_weighted.columns), axis=1)\n",
    "    # data_weighted = data_weighted.sort_index()\n",
    "    # # data_weighted = data_weighted.sort_values(by=\"sum\", ascending=False)\n",
    "    # sum_data_weighted_file = f\"{feature}-{model}-{mode}-impute={impute_value}_weighted_EU.csv\"\n",
    "    # data_weighted.to_csv(out_dir + sum_data_weighted_file)\n",
    "\n",
    "\n",
    "results_dir = \"/data/data1/conglab/jzhan6/CASP16/targetPDBs/Targets_oligo_interfaces_20240917/nr_interfaces/\"\n",
    "v1_file, v2_file = 'T1249v1o.nr_interface.results', 'T1249v2o.nr_interface.results'\n",
    "out_dir = \"./group_by_target_per_interface/\"\n",
    "model = \"best\"\n",
    "mode = \"all\"\n",
    "impute_value = -2\n",
    "group_by_target(results_dir, v1_file, v2_file, out_dir,\n",
    "                \"dockq_mean\", model, mode, impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file, out_dir,\n",
    "                \"qs_best_mean\", model, mode, impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file, out_dir,\n",
    "                \"qs_global_mean\", model, mode, impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file, out_dir,\n",
    "                \"ics_mean\", model, mode, impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file, out_dir,\n",
    "                \"ips_mean\", model, mode, impute_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
