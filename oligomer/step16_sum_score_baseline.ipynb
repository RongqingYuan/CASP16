{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home2/s439906/project/CASP16/oligomer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"./monomer_data_aug_30/processed/EU/\"\n",
    "csv_path = \"./monomer_data_Sep_10/processed/EU/\"\n",
    "csv_path = \"./monomer_data_Sep_10/raw_data/EU/\"\n",
    "# csv_path = \"./monomer_data_Sep_17/raw_data/\"\n",
    "csv_path = \"./oligomer_data_Sep_17/raw_data/\"\n",
    "sum_path = \"./sum_raw_new_EU/\"\n",
    "\n",
    "if not os.path.exists(sum_path):\n",
    "    os.makedirs(sum_path)\n",
    "\n",
    "csv_list = [txt for txt in os.listdir(\n",
    "    csv_path) if txt.endswith(\".csv\") and (txt.startswith(\"T1\") or txt.startswith(\"H1\"))]\n",
    "\n",
    "model = \"first\"\n",
    "model = \"best\"\n",
    "\n",
    "mode = \"hard\"\n",
    "mode = \"medium\"\n",
    "mode = \"easy\"\n",
    "mode = \"all\"\n",
    "\n",
    "\n",
    "\n",
    "# feature = \"GDT_TS\"\n",
    "# features = ['GDT_TS', 'GDT_HA', 'GDC_SC', 'GDC_ALL', 'RMS_CA', 'RMS_ALL', 'AL0_P',\n",
    "#             'AL4_P', 'ALI_P', 'LGA_S', 'RMSD[L]', 'MolPrb_Score', 'LDDT',\n",
    "#             'SphGr',\n",
    "#             'CAD_AA', 'RPF', 'TMscore', 'FlexE', 'QSE', 'CAD_SS', 'MP_clash',\n",
    "#             'MP_rotout', 'MP_ramout', 'MP_ramfv', 'reLLG_lddt', 'reLLG_const']\n",
    "\n",
    "# inverse_columns = [\"RMS_CA\", \"RMS_ALL\", \"err\",\n",
    "#                    \"RMSD[L]\", \"MolPrb_Score\", \"FlexE\", \"MP_clash\", \"MP_rotout\", \"MP_ramout\"]\n",
    "\n",
    "features = [\"QSglob\", \"QSbest\", \"ICS(F1)\", \"lDDT\", \"DockQ_Avg\",\n",
    "            \"IPS(JaccCoef)\", \"TMscore\"]\n",
    "feature = features[1]\n",
    "\n",
    "features = [\"qs_global\", \"qs_best\", \"ics\",\n",
    "            \"ips\", \"dockq_ave\", \"tm_score\", \"lddt\"]\n",
    "\n",
    "feature = features[0]\n",
    "\n",
    "\n",
    "def get_group_by_target(csv_path, csv_list, feature, model, mode):\n",
    "    data = pd.DataFrame()\n",
    "    data_raw = pd.DataFrame()\n",
    "    for csv_file in csv_list:\n",
    "        data_tmp = pd.read_csv(csv_path + csv_file, index_col=0)\n",
    "        data_tmp = pd.DataFrame(data_tmp[feature])\n",
    "        print(\"Processing {}\".format(csv_file), data_tmp.shape)\n",
    "        # breakpoint()\n",
    "        data_tmp.index = data_tmp.index.str.extract(\n",
    "            r'(\\w+)TS(\\w+)_(\\w+)').apply(lambda x: (f\"{x[0]}\", f\"TS{x[1]}\", x[2][0]), axis=1)\n",
    "        # breakpoint()\n",
    "        data_tmp.index = pd.MultiIndex.from_tuples(\n",
    "            data_tmp.index, names=['target', 'group', 'submission_id'])\n",
    "        # # get all data with submission_id == 6\n",
    "        # data_tmp = data_tmp.loc[(slice(None), slice(None), \"6\"), :]\n",
    "        # drop all data with submission_id == 6\n",
    "        if model == \"best\":\n",
    "            data_tmp = data_tmp.loc[(slice(None), slice(None), [\n",
    "                \"1\", \"2\", \"3\", \"4\", \"5\"]), :]\n",
    "        elif model == \"first\":\n",
    "            data_tmp = data_tmp.loc[(slice(None), slice(None),\n",
    "                                    \"1\"), :]\n",
    "        # grouped = data_tmp.groupby([\"group\", \"target\"])\n",
    "        # grouped = pd.DataFrame(grouped[feature].max())\n",
    "        # grouped.index = grouped.index.droplevel(1)\n",
    "\n",
    "        grouped = data_tmp.groupby([\"group\"])\n",
    "        grouped = pd.DataFrame(grouped[feature].max())\n",
    "        # grouped.index = grouped.index.droplevel(1)\n",
    "        # sort grouped\n",
    "        grouped = grouped.sort_values(by=feature, ascending=False)\n",
    "        initial_z = (grouped - grouped.mean()) / grouped.std()\n",
    "        new_z_score = pd.DataFrame(\n",
    "            index=grouped.index, columns=grouped.columns)\n",
    "        filtered_data = grouped[feature][initial_z[feature] >= -2]\n",
    "        new_mean = filtered_data.mean(skipna=True)\n",
    "        new_std = filtered_data.std(skipna=True)\n",
    "        new_z_score[feature] = (grouped[feature] - new_mean) / new_std\n",
    "        new_z_score = new_z_score.fillna(-2.0)\n",
    "        new_z_score = new_z_score.where(new_z_score > -2, -2)\n",
    "\n",
    "        # breakpoint()\n",
    "\n",
    "        # I actually don't understand why this is necessary... but need to keep it in mind.\n",
    "        # grouped = grouped.apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "        new_z_score = new_z_score.rename(\n",
    "            columns={feature: csv_file.split(\".\")[0]})\n",
    "        data = pd.concat([data, new_z_score], axis=1)\n",
    "        grouped = grouped.rename(\n",
    "            columns={feature: csv_file.split(\".\")[0]})\n",
    "        data_raw = pd.concat([data_raw, grouped], axis=1)\n",
    "    # impute data again with -2\n",
    "    # breakpoint()\n",
    "    data = data.fillna(-2.0)\n",
    "    data.to_csv(\"./group_by_target_EU/\" +\n",
    "                \"group_by_target-{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    data_raw.to_csv(\"./group_by_target_EU/\" +\n",
    "                    \"group_by_target_raw-{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    data[\"sum\"] = data.sum(axis=1)\n",
    "    data = data.sort_values(by=\"sum\", ascending=False)\n",
    "    data.to_csv(\"./sum/\" + \"sum_{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    return data, data_raw\n",
    "\n",
    "\n",
    "# data, data_raw = get_group_by_target(csv_path, csv_list, feature, model, mode)\n",
    "data_raw = pd.read_csv(\n",
    "    \"./group_by_target_EU_new/group_by_target_raw-{}-{}-{}.csv\".format(feature, model, mode), index_col=[0])\n",
    "# print the nan rate of the data in columns and rows\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# remove rows with more than 50% nan values\n",
    "data_raw = data_raw[data_raw.isna().sum(axis=1) < 0.5 * data_raw.shape[1]]\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# for each row, get its non-nan values in intersection with \"TS145\", and calculate the sum of the non-nan values\n",
    "# then divide the sum of the non-nan values by the sum of \"TS145\" to get the normalized sum\n",
    "# then plot the normalized sum\n",
    "data_raw = data_raw.T\n",
    "\n",
    "groups = data_raw.columns\n",
    "baseline_group = pd.DataFrame(data_raw[\"TS145\"])\n",
    "baseline_dict = {}\n",
    "for group in groups:\n",
    "    data_raw_group = pd.DataFrame(data_raw[group])\n",
    "    # get the intersection of non nan values in data_raw_group and baseline_group\n",
    "    non_nan_baseline = baseline_group[pd.notna(baseline_group[\"TS145\"])].index\n",
    "    non_nan_group = data_raw_group[pd.notna(data_raw_group[group])].index\n",
    "    # 计算两者的交集\n",
    "    intersection_index = non_nan_baseline.intersection(non_nan_group)\n",
    "    # 根据 intersection_index 获取对应的值\n",
    "    baseline_values = baseline_group.loc[intersection_index, \"TS145\"]\n",
    "    group_values = data_raw_group.loc[intersection_index, group]\n",
    "\n",
    "    # sum them up\n",
    "    sum_baseline = baseline_values.sum()\n",
    "    sum_group = group_values.sum()\n",
    "    ratio = sum_group / sum_baseline\n",
    "    baseline_dict[group] = ratio\n",
    "    # # 输出结果或进行进一步操作\n",
    "    # print(\n",
    "    #     f\"Group {group} has {len(intersection_index)} intersecting non-NaN values.\")\n",
    "    # print(\"Baseline values:\", baseline_values.values)\n",
    "    # print(f\"{group} values:\", group_values.values)\n",
    "\n",
    "# sort the baseline_dict by its values\n",
    "baseline_dict = dict(sorted(baseline_dict.items(),\n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "groups = [key[2:] for key in baseline_dict.keys()]\n",
    "values = list(baseline_dict.values())\n",
    "plt.figure(figsize=(30, 15))\n",
    "highlight_group = \"145\"\n",
    "bar_colors = ['C0' if group != highlight_group else 'C1' for group in groups]\n",
    "plt.bar(groups, values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=20, ha='right')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\n",
    "    \"sum of {} for {} model in {} targets, compared with baseline (group {}) in CASP16\".format(feature, model, mode, highlight_group), fontsize=20)\n",
    "plt.ylabel(\"sum of {}\".format(feature), fontsize=20)\n",
    "plt.axhline(y=1, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "for group, value in zip(groups, values):\n",
    "    if group == highlight_group:\n",
    "        plt.text(group, value + 0.02, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=20, color='C1')\n",
    "# first 5 groups\n",
    "first_5_groups = groups[:5]\n",
    "first_5_values = values[:5]\n",
    "for group, value in zip(first_5_groups, first_5_values):\n",
    "    plt.text(group, value + 0.01, str(value.round(2)),\n",
    "             ha='center', fontsize=10, color='C0')\n",
    "plt.savefig(\n",
    "    sum_path + \"sum_intersect_{}-{}-{}_with_colabfold_baseline.png\".format(feature, model, mode), dpi=300)\n",
    "###############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw=pd.read_csv(\"./group_by_target_EU_new/qs_global-best-all-impute=-2_raw.csv\", index_col=0)\n",
    "# print the nan rate of the data in columns and rows\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# remove rows with more than 50% nan values\n",
    "data_raw = data_raw[data_raw.isna().sum(axis=1) < 0.5 * data_raw.shape[1]]\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# for each row, get its non-nan values in intersection with \"TS145\", and calculate the sum of the non-nan values\n",
    "# then divide the sum of the non-nan values by the sum of \"TS145\" to get the normalized sum\n",
    "# then plot the normalized sum\n",
    "data_raw = data_raw.T\n",
    "\n",
    "groups = data_raw.columns\n",
    "baseline_group = pd.DataFrame(data_raw[\"TS145\"])\n",
    "baseline_dict = {}\n",
    "for group in groups:\n",
    "    data_raw_group = pd.DataFrame(data_raw[group])\n",
    "    # get the intersection of non nan values in data_raw_group and baseline_group\n",
    "    non_nan_baseline = baseline_group[pd.notna(baseline_group[\"TS145\"])].index\n",
    "    non_nan_group = data_raw_group[pd.notna(data_raw_group[group])].index\n",
    "    # 计算两者的交集\n",
    "    intersection_index = non_nan_baseline.intersection(non_nan_group)\n",
    "    # 根据 intersection_index 获取对应的值\n",
    "    baseline_values = baseline_group.loc[intersection_index, \"TS145\"]\n",
    "    group_values = data_raw_group.loc[intersection_index, group]\n",
    "\n",
    "    # sum them up\n",
    "    sum_baseline = baseline_values.sum()\n",
    "    sum_group = group_values.sum()\n",
    "    ratio = sum_group / sum_baseline\n",
    "    baseline_dict[group] = ratio\n",
    "    # # 输出结果或进行进一步操作\n",
    "    # print(\n",
    "    #     f\"Group {group} has {len(intersection_index)} intersecting non-NaN values.\")\n",
    "    # print(\"Baseline values:\", baseline_values.values)\n",
    "    # print(f\"{group} values:\", group_values.values)\n",
    "\n",
    "# sort the baseline_dict by its values\n",
    "baseline_dict = dict(sorted(baseline_dict.items(),\n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "groups = [key[2:] for key in baseline_dict.keys()]\n",
    "values = list(baseline_dict.values())\n",
    "plt.figure(figsize=(30, 10),dpi=300)\n",
    "highlight_group = \"145\"\n",
    "bar_colors = ['C0' if group != highlight_group else 'C1' for group in groups]\n",
    "plt.bar(groups, values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=20, ha='right')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\n",
    "    \"sum of {} for {} model in {} targets, compared with baseline (group {}) in CASP16\".format(feature, model, mode, highlight_group), fontsize=20)\n",
    "plt.ylabel(\"sum of {}\".format(feature), fontsize=20)\n",
    "plt.axhline(y=1, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "for group, value in zip(groups, values):\n",
    "    if group == highlight_group:\n",
    "        plt.text(group, value + 0.02, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=20, color='C1')\n",
    "# first 5 groups\n",
    "first_5_groups = groups[:5]\n",
    "first_5_values = values[:5]\n",
    "for group, value in zip(first_5_groups, first_5_values):\n",
    "    plt.text(group, value + 0.01, str(value.round(2)),\n",
    "             ha='center', fontsize=10, color='C0')\n",
    "plt.savefig(\n",
    "    sum_path + \"sum_intersect_{}-{}-{}_with_colabfold_baseline.png\".format(feature, model, mode), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw=pd.read_csv(\"./group_by_target_EU_CASP15/group_by_target_raw-QSbest-best-all.csv\", index_col=0)\n",
    "# print the nan rate of the data in columns and rows\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# remove rows with more than 50% nan values\n",
    "data_raw = data_raw[data_raw.isna().sum(axis=1) < 0.5 * data_raw.shape[1]]\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# for each row, get its non-nan values in intersection with \"TS145\", and calculate the sum of the non-nan values\n",
    "# then divide the sum of the non-nan values by the sum of \"TS145\" to get the normalized sum\n",
    "# then plot the normalized sum\n",
    "data_raw = data_raw.T\n",
    "\n",
    "groups = data_raw.columns\n",
    "baseline_group = pd.DataFrame(data_raw[\"TS446\"])\n",
    "baseline_dict = {}\n",
    "for group in groups:\n",
    "    data_raw_group = pd.DataFrame(data_raw[group])\n",
    "    # get the intersection of non nan values in data_raw_group and baseline_group\n",
    "    non_nan_baseline = baseline_group[pd.notna(baseline_group[\"TS446\"])].index\n",
    "    non_nan_group = data_raw_group[pd.notna(data_raw_group[group])].index\n",
    "    # 计算两者的交集\n",
    "    intersection_index = non_nan_baseline.intersection(non_nan_group)\n",
    "    # 根据 intersection_index 获取对应的值\n",
    "    baseline_values = baseline_group.loc[intersection_index, \"TS446\"]\n",
    "    group_values = data_raw_group.loc[intersection_index, group]\n",
    "\n",
    "    # sum them up\n",
    "    sum_baseline = baseline_values.sum()\n",
    "    sum_group = group_values.sum()\n",
    "    ratio = sum_group / sum_baseline\n",
    "    baseline_dict[group] = ratio\n",
    "    # # 输出结果或进行进一步操作\n",
    "    # print(\n",
    "    #     f\"Group {group} has {len(intersection_index)} intersecting non-NaN values.\")\n",
    "    # print(\"Baseline values:\", baseline_values.values)\n",
    "    # print(f\"{group} values:\", group_values.values)\n",
    "\n",
    "# sort the baseline_dict by its values\n",
    "baseline_dict = dict(sorted(baseline_dict.items(),\n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "groups = [key[2:] for key in baseline_dict.keys()]\n",
    "values = list(baseline_dict.values())\n",
    "plt.figure(figsize=(30, 10),dpi=300)\n",
    "highlight_group = \"446\"\n",
    "bar_colors = ['C0' if group != highlight_group else 'C1' for group in groups]\n",
    "plt.bar(groups, values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=20, ha='right')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\n",
    "    \"sum of {} for {} model in {} targets, compared with baseline (group {}) in CASP15\".format(feature, model, mode, highlight_group), fontsize=20)\n",
    "plt.ylabel(\"sum of {}\".format(feature), fontsize=20)\n",
    "plt.axhline(y=1, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "for group, value in zip(groups, values):\n",
    "    if group == highlight_group:\n",
    "        plt.text(group, value + 0.02, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=20, color='C1')\n",
    "# first 5 groups\n",
    "first_5_groups = groups[:5]\n",
    "first_5_values = values[:5]\n",
    "for group, value in zip(first_5_groups, first_5_values):\n",
    "    plt.text(group, value + 0.01, str(value.round(2)),\n",
    "             ha='center', fontsize=10, color='C0')\n",
    "plt.savefig(\n",
    "    sum_path + \"sum_intersect_{}-{}-{}_with_colabfold_baseline.png\".format(feature, model, mode), dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
