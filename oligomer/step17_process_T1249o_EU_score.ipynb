{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"/home2/s439906/project/CASP16/oligomer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"/data/data1/conglab/jzhan6/CASP16/targetPDBs/Targets_oligo_interfaces_20240917/model_results/\"\n",
    "v1_file, v2_file = 'T1249v1o.results', 'T1249v2o.results'\n",
    "out_dir = \"./group_by_target_EU_new/\"\n",
    "model = \"best\"\n",
    "mode = \"all\"\n",
    "impute_value = -2\n",
    "\n",
    "\n",
    "def group_by_target(results_dir, v1_file, v2_file, out_dir, feature, model, mode, impute_value=-2):\n",
    "    print(\"Processing {}\".format(feature))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    # data = pd.DataFrame()\n",
    "    # data_raw = pd.DataFrame()\n",
    "    v1_path = results_dir + v1_file\n",
    "    # it is actually a tsv file, us pd.read_csv to read it\n",
    "    data_v1 = pd.read_csv(v1_path, sep=\"\\t\", index_col=0)\n",
    "    data_v1 = data_v1.replace(\"-\", np.nan)\n",
    "    data_v1[feature] = data_v1[feature].astype(float)\n",
    "    # fill \"-\" with nan\n",
    "    data_v1 = pd.DataFrame(data_v1[feature])\n",
    "    data_v1.index = data_v1.index.str.extract(\n",
    "        r'(\\w+)TS(\\w+)_(\\w+)').apply(lambda x: (f\"{x[0]}\", f\"TS{x[1]}\", x[2][0]), axis=1)\n",
    "    data_v1.index = pd.MultiIndex.from_tuples(\n",
    "        data_v1.index, names=['target', 'group', 'submission_id'])\n",
    "    if model == \"best\":\n",
    "        data_v1 = data_v1.loc[(slice(None), slice(None), [\n",
    "            \"1\", \"2\", \"3\", \"4\", \"5\"]), :]\n",
    "    elif model == \"first\":\n",
    "        data_v1 = data_v1.loc[(slice(None), slice(None),\n",
    "                               \"1\"), :]\n",
    "    data_v1_grouped = data_v1.groupby([\"target\", \"group\"])\n",
    "    data_v1_grouped = pd.DataFrame(data_v1_grouped[feature].max())\n",
    "    print(data_v1_grouped)\n",
    "    # get target = T1249v1 rows\n",
    "    target_v1_model_v1 = data_v1_grouped.loc[\"T1249v1\", :]\n",
    "    # rename column to v1_file\n",
    "    target_v1_model_v1 = target_v1_model_v1.rename(\n",
    "        columns={feature: f\"{feature}_v1\"})\n",
    "    print(target_v1_model_v1)\n",
    "    target_v1_model_v2 = data_v1_grouped.loc[\"T1249v2\", :]\n",
    "    target_v1_model_v2 = target_v1_model_v2.rename(\n",
    "        columns={feature: f\"{feature}_v1\"})\n",
    "\n",
    "    v2_path = results_dir + v2_file\n",
    "    # it is actually a tsv file, us pd.read_csv to read it\n",
    "    data_v2 = pd.read_csv(v2_path, sep=\"\\t\", index_col=0)\n",
    "    data_v2 = data_v2.replace(\"-\", np.nan)\n",
    "    data_v2[feature] = data_v2[feature].astype(float)\n",
    "    # fill \"-\" with nan\n",
    "    data_v2 = pd.DataFrame(data_v2[feature])\n",
    "    data_v2.index = data_v2.index.str.extract(\n",
    "        r'(\\w+)TS(\\w+)_(\\w+)').apply(lambda x: (f\"{x[0]}\", f\"TS{x[1]}\", x[2][0]), axis=1)\n",
    "    data_v2.index = pd.MultiIndex.from_tuples(\n",
    "        data_v2.index, names=['target', 'group', 'submission_id'])\n",
    "    if model == \"best\":\n",
    "        data_v2 = data_v2.loc[(slice(None), slice(None), [\n",
    "            \"1\", \"2\", \"3\", \"4\", \"5\"]), :]\n",
    "    elif model == \"first\":\n",
    "        data_v2 = data_v2.loc[(slice(None), slice(None),\n",
    "                               \"1\"), :]\n",
    "    data_v2_grouped = data_v2.groupby([\"target\", \"group\"])\n",
    "    data_v2_grouped = pd.DataFrame(data_v2_grouped[feature].max())\n",
    "    # get target = T1249v1 rows\n",
    "    target_v2_model_v1 = data_v2_grouped.loc[\"T1249v1\", :]\n",
    "    # rename column to v1_file\n",
    "    target_v2_model_v1 = target_v2_model_v1.rename(\n",
    "        columns={feature: f\"{feature}_v2\"})\n",
    "    target_v2_model_v2 = data_v2_grouped.loc[\"T1249v2\", :]\n",
    "    target_v2_model_v2 = target_v2_model_v2.rename(\n",
    "        columns={feature: f\"{feature}_v2\"})\n",
    "\n",
    "    case_1 = pd.concat([target_v1_model_v1, target_v2_model_v2], axis=1)\n",
    "    case_2 = pd.concat([target_v1_model_v2, target_v2_model_v1], axis=1)\n",
    "    # this is a combinatorial problem, we need to choose the best one with restrictions\n",
    "    # fortunately there are only 2 combinations...\n",
    "    case_1[\"sum\"] = case_1.sum(axis=1)\n",
    "    case_2[\"sum\"] = case_2.sum(axis=1)\n",
    "\n",
    "    # choose the best one for each group\n",
    "    best_df = pd.DataFrame()\n",
    "    for group in case_1.index:\n",
    "        if case_1.loc[group, \"sum\"] > case_2.loc[group, \"sum\"]:\n",
    "            best_df = pd.concat([best_df, case_1.loc[group, :].to_frame().T])\n",
    "        else:\n",
    "            best_df = pd.concat([best_df, case_2.loc[group, :].to_frame().T])\n",
    "\n",
    "    best_df = best_df.drop(columns=[\"sum\"])\n",
    "    best_df = best_df.rename(columns={f\"{feature}_v1\": v1_file.split(\".\")[\n",
    "                             0], f\"{feature}_v2\": v2_file.split(\".\")[0]})\n",
    "    best_df.to_csv(out_dir +\n",
    "                   \"group_by_target-raw-T1249o-{}-{}-{}-impute_value={}.csv\".format(feature, model, mode, impute_value))\n",
    "    print(best_df)\n",
    "    for target in best_df.columns:\n",
    "        data = best_df[target]\n",
    "        data = pd.DataFrame(data)\n",
    "        initial_z = (data - data.mean()) / data.std()\n",
    "        new_z_score = pd.DataFrame(\n",
    "            index=data.index, columns=data.columns)\n",
    "        filtered_data = data[initial_z >= -2]\n",
    "        new_mean = filtered_data.mean(skipna=True)\n",
    "        new_std = filtered_data.std(skipna=True)\n",
    "        new_z_score[target] = (data - new_mean) / new_std\n",
    "        new_z_score = new_z_score.fillna(impute_value)\n",
    "        new_z_score = new_z_score.where(\n",
    "            new_z_score > impute_value, impute_value)\n",
    "        best_df[target] = new_z_score\n",
    "    best_df.to_csv(out_dir +\n",
    "                   \"group_by_target-T1249o-{}-{}-{}-impute_value={}.csv\".format(feature, model, mode, impute_value))\n",
    "\n",
    "    print(best_df)\n",
    "\n",
    "    template_raw_file = out_dir + \\\n",
    "        f\"{feature}-{model}-{mode}-impute={impute_value}_raw.csv\"\n",
    "    template_EU_file = out_dir + \\\n",
    "        f\"{feature}-{model}-{mode}-impute={impute_value}.csv\"\n",
    "    template_raw = pd.read_csv(template_raw_file, index_col=0)\n",
    "    template_EU = pd.read_csv(template_EU_file, index_col=0)\n",
    "    print(template_raw)\n",
    "    print(template_EU)\n",
    "    template_raw = pd.concat([template_raw, best_df], axis=1)\n",
    "    template_EU = pd.concat([template_EU, best_df], axis=1)\n",
    "    # impute template_EU with impute_value\n",
    "    template_EU = template_EU.fillna(impute_value)\n",
    "    # sort row and column alphabetically\n",
    "    template_EU = template_EU.reindex(sorted(template_EU.columns), axis=1)\n",
    "    template_EU = template_EU.sort_index()\n",
    "    template_raw = template_raw.reindex(sorted(template_raw.columns), axis=1)\n",
    "    template_raw = template_raw.sort_index()\n",
    "    # drop TS314 row in the dataframe\n",
    "    # this is a special case for T1249o\n",
    "    template_raw = template_raw.drop(index='TS314')\n",
    "    template_EU = template_EU.drop(index='TS314')\n",
    "    template_raw.to_csv(template_raw_file)\n",
    "    template_EU.to_csv(template_EU_file)\n",
    "\n",
    "    # breakpoint()\n",
    "    # filtered_data = best_df[feature][initial_z[feature] >= -2]\n",
    "    # new_mean = filtered_data.mean(skipna=True)\n",
    "    # new_std = filtered_data.std(skipna=True)\n",
    "    # new_z_score[feature] = (best_df[feature] - new_mean) / new_std\n",
    "    # new_z_score = new_z_score.fillna(-2.0)\n",
    "    # new_z_score = new_z_score.where(new_z_score > -2, -2)\n",
    "    # new_z_score = new_z_score.rename(\n",
    "    #     columns={feature:     v1_file.split(\".\")[0]})\n",
    "    # best_df = best_df.rename(\n",
    "    #     columns={feature:     v1_file.split(\".\")[0]})\n",
    "    # breakpoint()\n",
    "    # grouped = grouped.sort_values(by=feature, ascending=False)\n",
    "    # initial_z = (grouped - grouped.mean()) / grouped.std()\n",
    "    # new_z_score = pd.DataFrame(\n",
    "    #     index=grouped.index, columns=grouped.columns)\n",
    "    # filtered_data = grouped[feature][initial_z[feature] >= -2]\n",
    "    # new_mean = filtered_data.mean(skipna=True)\n",
    "    # new_std = filtered_data.std(skipna=True)\n",
    "    # new_z_score[feature] = (grouped[feature] - new_mean) / new_std\n",
    "    # new_z_score = new_z_score.fillna(-2.0)\n",
    "    # new_z_score = new_z_score.where(new_z_score > -2, -2)\n",
    "    # new_z_score = new_z_score.rename(\n",
    "    #     columns={feature:     v1_file.split(\".\")[0]})\n",
    "    # data = pd.concat([data, new_z_score], axis=1)\n",
    "    # grouped = grouped.rename(\n",
    "    #     columns={feature:     v1_file.split(\".\")[0]})\n",
    "    # data_raw = pd.concat([data_raw, grouped], axis=1)\n",
    "\n",
    "    # data = data.fillna(-2.0)\n",
    "    # data.to_csv(out_dir +\n",
    "    #             \"group_by_target-{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    # data_raw.to_csv(out_dir +\n",
    "    #                 \"group_by_target_raw-{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    # data[\"sum\"] = data.sum(axis=1)\n",
    "    # data = data.sort_values(by=\"sum\", ascending=False)\n",
    "    # data.to_csv(out_dir + \"sum_{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "\n",
    "group_by_target(results_dir, v1_file, v2_file,\n",
    "                out_dir, \"tm_score\", model, mode, impute_value=impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file,\n",
    "                out_dir, \"lddt\", model, mode, impute_value=impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file,\n",
    "                out_dir, \"qs_global\", model, mode, impute_value=impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file,\n",
    "                out_dir, \"qs_best\", model, mode, impute_value=impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file,\n",
    "                out_dir, \"ics\", model, mode, impute_value=impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file,\n",
    "                out_dir, \"ips\", model, mode, impute_value=impute_value)\n",
    "group_by_target(results_dir, v1_file, v2_file,\n",
    "                out_dir, \"dockq_ave\", model, mode, impute_value=impute_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
