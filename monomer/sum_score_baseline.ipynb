{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home2/s439906/project/CASP16/monomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing T1270-D2.csv\n",
      "Processing T1271s3-D1.csv\n",
      "Processing T1259-D1.csv\n",
      "Processing T1231-D1.csv\n",
      "Processing T1271s7-D1.csv\n",
      "Processing T1244s1-D1.csv\n",
      "Processing T1245s1-D1.csv\n",
      "Processing T1201-D1.csv\n",
      "Processing T1218-D3.csv\n",
      "Processing T1280-D1.csv\n",
      "Processing T1270-D1.csv\n",
      "Processing T1269-D2.csv\n",
      "Processing T1271s4-D1.csv\n",
      "Processing T1279-D1.csv\n",
      "Processing T1206-D1.csv\n",
      "Processing T1276-D1.csv\n",
      "Processing T1257-D1.csv\n",
      "Processing T1292-D1.csv\n",
      "Processing T1207-D1.csv\n",
      "Processing T1239-D2-all.csv\n",
      "Processing T1249v1-D1.csv\n",
      "Processing T1228-D2-all.csv\n",
      "Processing T1274-D1.csv\n",
      "Processing T1271s5-D2.csv\n",
      "Processing T1267s1-D2.csv\n",
      "Processing T1218-D1.csv\n",
      "Processing T1298-D1.csv\n",
      "Processing T1272s6-D1.csv\n",
      "Processing T1272s8-D1.csv\n",
      "Processing T1271s8-D2.csv\n",
      "Processing T1267s2-D1.csv\n",
      "Processing T1246-D1.csv\n",
      "Processing T1212-D1.csv\n",
      "Processing T1234-D1.csv\n",
      "Processing T1235-D1.csv\n",
      "Processing T1210-D1.csv\n",
      "Processing T1240-D2.csv\n",
      "Processing T1243-D1.csv\n",
      "Processing T1284-D1.csv\n",
      "Processing T1294-D1-all.csv\n",
      "Processing T1295-D1.csv\n",
      "Processing T1269-D1.csv\n",
      "Processing T1239-D1-all.csv\n",
      "Processing T1226-D1.csv\n",
      "Processing T1269-D3.csv\n",
      "Processing T1208s2-D1.csv\n",
      "Processing T1228-D1-all.csv\n",
      "Processing T1267s1-D1.csv\n",
      "Processing T1227s1-D1.csv\n",
      "Processing T1271s8-D1.csv\n",
      "Processing T1272s9-D1.csv\n",
      "Processing T1228-D3-all.csv\n",
      "Processing T1271s1-D1.csv\n",
      "Processing T1299-D1.csv\n",
      "Processing T1220s1-D1.csv\n",
      "Processing T1228-D4-all.csv\n",
      "Processing T1239-D3-all.csv\n",
      "Processing T1298-D2.csv\n",
      "Processing T1271s2-D1.csv\n",
      "Processing T1240-D1.csv\n",
      "Processing T1271s6-D1.csv\n",
      "Processing T1295-D2.csv\n",
      "Processing T1278-D1.csv\n",
      "Processing T1218-D2.csv\n",
      "Processing T1239-D4-all.csv\n",
      "Processing T1237-D1.csv\n",
      "Processing T1266-D1.csv\n",
      "Processing T1208s1-D1.csv\n",
      "Processing T1295-D3.csv\n",
      "Processing T1279-D2.csv\n",
      "Processing T1271s5-D1.csv\n",
      "Processing T1272s2-D1.csv\n",
      "Processing T1245s2-D1.csv\n",
      "Processing T1230s1-D1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1201-D1</th>\n",
       "      <th>T1206-D1</th>\n",
       "      <th>T1207-D1</th>\n",
       "      <th>T1208s1-D1</th>\n",
       "      <th>T1208s2-D1</th>\n",
       "      <th>T1210-D1</th>\n",
       "      <th>T1212-D1</th>\n",
       "      <th>T1218-D1</th>\n",
       "      <th>T1218-D2</th>\n",
       "      <th>T1218-D3</th>\n",
       "      <th>...</th>\n",
       "      <th>T1280-D1</th>\n",
       "      <th>T1284-D1</th>\n",
       "      <th>T1292-D1</th>\n",
       "      <th>T1294-D1-all</th>\n",
       "      <th>T1295-D1</th>\n",
       "      <th>T1295-D2</th>\n",
       "      <th>T1295-D3</th>\n",
       "      <th>T1298-D1</th>\n",
       "      <th>T1298-D2</th>\n",
       "      <th>T1299-D1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TS456</th>\n",
       "      <td>73.78</td>\n",
       "      <td>92.61</td>\n",
       "      <td>69.88</td>\n",
       "      <td>85.98</td>\n",
       "      <td>92.24</td>\n",
       "      <td>47.50</td>\n",
       "      <td>69.53</td>\n",
       "      <td>73.06</td>\n",
       "      <td>70.97</td>\n",
       "      <td>86.01</td>\n",
       "      <td>...</td>\n",
       "      <td>92.20</td>\n",
       "      <td>68.91</td>\n",
       "      <td>97.41</td>\n",
       "      <td>89.86</td>\n",
       "      <td>60.78</td>\n",
       "      <td>86.23</td>\n",
       "      <td>71.16</td>\n",
       "      <td>77.73</td>\n",
       "      <td>78.39</td>\n",
       "      <td>84.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS052</th>\n",
       "      <td>74.69</td>\n",
       "      <td>93.04</td>\n",
       "      <td>50.00</td>\n",
       "      <td>83.57</td>\n",
       "      <td>92.40</td>\n",
       "      <td>50.02</td>\n",
       "      <td>74.14</td>\n",
       "      <td>74.41</td>\n",
       "      <td>70.97</td>\n",
       "      <td>85.82</td>\n",
       "      <td>...</td>\n",
       "      <td>92.20</td>\n",
       "      <td>69.75</td>\n",
       "      <td>97.80</td>\n",
       "      <td>89.86</td>\n",
       "      <td>60.31</td>\n",
       "      <td>87.68</td>\n",
       "      <td>71.16</td>\n",
       "      <td>77.73</td>\n",
       "      <td>78.39</td>\n",
       "      <td>82.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS110</th>\n",
       "      <td>78.05</td>\n",
       "      <td>93.26</td>\n",
       "      <td>48.36</td>\n",
       "      <td>83.01</td>\n",
       "      <td>88.64</td>\n",
       "      <td>45.21</td>\n",
       "      <td>63.04</td>\n",
       "      <td>74.28</td>\n",
       "      <td>77.38</td>\n",
       "      <td>85.45</td>\n",
       "      <td>...</td>\n",
       "      <td>91.67</td>\n",
       "      <td>72.48</td>\n",
       "      <td>97.02</td>\n",
       "      <td>88.65</td>\n",
       "      <td>78.44</td>\n",
       "      <td>86.78</td>\n",
       "      <td>73.56</td>\n",
       "      <td>72.69</td>\n",
       "      <td>77.03</td>\n",
       "      <td>83.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS345</th>\n",
       "      <td>74.69</td>\n",
       "      <td>93.59</td>\n",
       "      <td>47.75</td>\n",
       "      <td>81.41</td>\n",
       "      <td>86.60</td>\n",
       "      <td>49.14</td>\n",
       "      <td>66.80</td>\n",
       "      <td>75.55</td>\n",
       "      <td>72.69</td>\n",
       "      <td>85.63</td>\n",
       "      <td>...</td>\n",
       "      <td>90.17</td>\n",
       "      <td>80.46</td>\n",
       "      <td>98.06</td>\n",
       "      <td>89.37</td>\n",
       "      <td>62.81</td>\n",
       "      <td>79.53</td>\n",
       "      <td>74.20</td>\n",
       "      <td>77.31</td>\n",
       "      <td>73.42</td>\n",
       "      <td>84.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS022</th>\n",
       "      <td>73.93</td>\n",
       "      <td>93.59</td>\n",
       "      <td>67.22</td>\n",
       "      <td>83.41</td>\n",
       "      <td>91.83</td>\n",
       "      <td>46.15</td>\n",
       "      <td>73.45</td>\n",
       "      <td>76.94</td>\n",
       "      <td>70.97</td>\n",
       "      <td>85.82</td>\n",
       "      <td>...</td>\n",
       "      <td>92.20</td>\n",
       "      <td>69.75</td>\n",
       "      <td>97.80</td>\n",
       "      <td>89.86</td>\n",
       "      <td>60.31</td>\n",
       "      <td>86.05</td>\n",
       "      <td>71.16</td>\n",
       "      <td>77.73</td>\n",
       "      <td>78.39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS044</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS281</th>\n",
       "      <td>50.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS355</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TS138</th>\n",
       "      <td>15.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       T1201-D1  T1206-D1  T1207-D1  T1208s1-D1  T1208s2-D1  T1210-D1  \\\n",
       "TS456     73.78     92.61     69.88       85.98       92.24     47.50   \n",
       "TS052     74.69     93.04     50.00       83.57       92.40     50.02   \n",
       "TS110     78.05     93.26     48.36       83.01       88.64     45.21   \n",
       "TS345     74.69     93.59     47.75       81.41       86.60     49.14   \n",
       "TS022     73.93     93.59     67.22       83.41       91.83     46.15   \n",
       "...         ...       ...       ...         ...         ...       ...   \n",
       "TS044       NaN       NaN       NaN         NaN         NaN       NaN   \n",
       "TS281     50.92       NaN       NaN         NaN         NaN       NaN   \n",
       "TS355       NaN       NaN     47.75         NaN         NaN       NaN   \n",
       "TS197       NaN       NaN     47.75         NaN         NaN       NaN   \n",
       "TS138     15.70       NaN     25.41         NaN         NaN       NaN   \n",
       "\n",
       "       T1212-D1  T1218-D1  T1218-D2  T1218-D3  ...  T1280-D1  T1284-D1  \\\n",
       "TS456     69.53     73.06     70.97     86.01  ...     92.20     68.91   \n",
       "TS052     74.14     74.41     70.97     85.82  ...     92.20     69.75   \n",
       "TS110     63.04     74.28     77.38     85.45  ...     91.67     72.48   \n",
       "TS345     66.80     75.55     72.69     85.63  ...     90.17     80.46   \n",
       "TS022     73.45     76.94     70.97     85.82  ...     92.20     69.75   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "TS044     64.97       NaN       NaN       NaN  ...       NaN       NaN   \n",
       "TS281       NaN       NaN       NaN       NaN  ...       NaN       NaN   \n",
       "TS355       NaN       NaN       NaN       NaN  ...       NaN       NaN   \n",
       "TS197       NaN       NaN       NaN       NaN  ...       NaN       NaN   \n",
       "TS138       NaN       NaN       NaN       NaN  ...       NaN       NaN   \n",
       "\n",
       "       T1292-D1  T1294-D1-all  T1295-D1  T1295-D2  T1295-D3  T1298-D1  \\\n",
       "TS456     97.41         89.86     60.78     86.23     71.16     77.73   \n",
       "TS052     97.80         89.86     60.31     87.68     71.16     77.73   \n",
       "TS110     97.02         88.65     78.44     86.78     73.56     72.69   \n",
       "TS345     98.06         89.37     62.81     79.53     74.20     77.31   \n",
       "TS022     97.80         89.86     60.31     86.05     71.16     77.73   \n",
       "...         ...           ...       ...       ...       ...       ...   \n",
       "TS044       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "TS281       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "TS355       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "TS197       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "TS138       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "       T1298-D2  T1299-D1  \n",
       "TS456     78.39     84.08  \n",
       "TS052     78.39     82.89  \n",
       "TS110     77.03     83.78  \n",
       "TS345     73.42     84.08  \n",
       "TS022     78.39       NaN  \n",
       "...         ...       ...  \n",
       "TS044       NaN       NaN  \n",
       "TS281       NaN       NaN  \n",
       "TS355       NaN       NaN  \n",
       "TS197       NaN       NaN  \n",
       "TS138       NaN       NaN  \n",
       "\n",
       "[113 rows x 74 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv_path = \"./monomer_data_aug_30/processed/EU/\"\n",
    "# csv_path = \"./monomer_data_Sep_10/processed/EU/\"\n",
    "# csv_path = \"./monomer_data_Sep_10/raw_data/EU/\"\n",
    "csv_path = \"./monomer_data_Sep_15_EU/raw_data/\"\n",
    "# baseline_path = \"./baseline_data_Sep_15_EU/raw_data/\"\n",
    "csv_list = [txt for txt in os.listdir(\n",
    "    csv_path) if txt.endswith(\".csv\") and txt.startswith(\"T1\")]\n",
    "out_path = \"./group_by_target/\"\n",
    "sum_path = \"./sum/\"\n",
    "png_path = \"./png/\"\n",
    "\n",
    "# out_path = \"./group_by_target/\"\n",
    "# sum_path = \"./sum/\"\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "if not os.path.exists(sum_path):\n",
    "    os.makedirs(sum_path)\n",
    "if not os.path.exists(png_path):\n",
    "    os.makedirs(png_path)\n",
    "\n",
    "model = \"first\"\n",
    "model = \"best\"\n",
    "\n",
    "mode = \"easy\"\n",
    "mode = \"medium\"\n",
    "mode = \"hard\"\n",
    "mode = \"all\"\n",
    "\n",
    "\n",
    "\n",
    "# print(csv_list.__len__())\n",
    "# breakpoint()\n",
    "# read all data and concatenate them into one big dataframe\n",
    "feature = \"GDT_TS\"\n",
    "feature = \"GDT_HA\"\n",
    "features = ['GDT_TS',\n",
    "            'GDT_HA', 'GDC_SC', 'GDC_ALL', 'RMS_CA', 'RMS_ALL', 'AL0_P',\n",
    "            'AL4_P', 'ALI_P', 'LGA_S', 'RMSD[L]', 'MolPrb_Score', 'LDDT',\n",
    "            'SphGr',\n",
    "            'CAD_AA', 'RPF', 'TMscore', 'FlexE', 'QSE', 'CAD_SS', 'MP_clash',\n",
    "            'MP_rotout', 'MP_ramout', 'MP_ramfv', 'reLLG_lddt', 'reLLG_const']\n",
    "\n",
    "inverse_columns = [\"RMS_CA\", \"RMS_ALL\", \"err\",\n",
    "                   \"RMSD[L]\", \"MolPrb_Score\", \"FlexE\", \"MP_clash\", \"MP_rotout\", \"MP_ramout\"]\n",
    "\n",
    "\n",
    "def get_group_by_target(csv_list, csv_path, feature, model, mode):\n",
    "    data = pd.DataFrame()\n",
    "    data_raw = pd.DataFrame()  # store raw data for analysis\n",
    "    for csv_file in csv_list:\n",
    "        print(\"Processing {}\".format(csv_file))\n",
    "        data_tmp = pd.read_csv(csv_path + csv_file, index_col=0)\n",
    "        data_tmp = pd.DataFrame(data_tmp[feature])\n",
    "        # there is a BUG here. something wrong with group 999 processing but I don't have time to fix it.\n",
    "        # so just leave it here as it is, since I even know which group is colabfold lol :D\n",
    "        data_tmp = data_tmp.replace(\"-\", float(0))\n",
    "        if feature in inverse_columns:\n",
    "            data_tmp[feature] = -data_tmp[feature]\n",
    "        data_tmp.index = data_tmp.index.str.extract(\n",
    "            r'(T\\w+)TS(\\w+)_(\\w+)-(D\\w+)').apply(lambda x: (f\"{x[0]}-{x[3]}\", f\"TS{x[1]}\", x[2]), axis=1)\n",
    "        data_tmp.index = pd.MultiIndex.from_tuples(\n",
    "            data_tmp.index, names=['target', 'group', 'submission_id'])\n",
    "        # # get all data with submission_id == 6\n",
    "        # data_tmp = data_tmp.loc[(slice(None), slice(None), \"6\"), :]\n",
    "        # drop all data with submission_id == 6\n",
    "        if model == \"best\":\n",
    "            data_tmp = data_tmp.loc[(slice(None), slice(None), [\n",
    "                \"1\", \"2\", \"3\", \"4\", \"5\"]), :]  # remove 6\n",
    "        elif model == \"first\":\n",
    "            data_tmp = data_tmp.loc[(slice(None), slice(None),\n",
    "                                     \"1\"), :]\n",
    "        grouped = data_tmp.groupby([\"group\"])\n",
    "        grouped = pd.DataFrame(grouped[feature].max())\n",
    "        # grouped.index = grouped.index.droplevel(1)\n",
    "        # if there is any value in this column that is a string, convert it to float\n",
    "        grouped[feature] = grouped[feature].astype(float)\n",
    "        try:\n",
    "            grouped = grouped.sort_values(by=feature, ascending=False)\n",
    "            # print all the value in grouped\n",
    "            # for i in range(grouped.shape[0]):print(grouped.iloc[i])\n",
    "            initial_z = (grouped - grouped.mean()) / grouped.std()\n",
    "        except:\n",
    "            breakpoint()\n",
    "        new_z_score = pd.DataFrame(\n",
    "            index=grouped.index, columns=grouped.columns)\n",
    "        filtered_data = grouped[feature][initial_z[feature] >= -2]\n",
    "        new_mean = filtered_data.mean(skipna=True)\n",
    "        new_std = filtered_data.std(skipna=True)\n",
    "        new_z_score[feature] = (grouped[feature] - new_mean) / new_std\n",
    "        new_z_score = new_z_score.fillna(-2.0)\n",
    "        new_z_score = new_z_score.where(new_z_score > -2, -2)\n",
    "\n",
    "        # I actually don't understand why this is necessary... but need to keep it in mind.\n",
    "        # grouped = grouped.apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "        new_z_score = new_z_score.rename(\n",
    "            columns={feature: csv_file.split(\".\")[0]})\n",
    "        grouped = grouped.rename(\n",
    "            columns={feature: csv_file.split(\".\")[0]})\n",
    "        data = pd.concat([data, new_z_score], axis=1)\n",
    "        data_raw = pd.concat([data_raw, grouped], axis=1)\n",
    "\n",
    "    # breakpoint()\n",
    "    # impute data again with -2\n",
    "    data = data.fillna(-2.0)\n",
    "    # sort columns by alphabetical order\n",
    "    data = data.reindex(sorted(data.columns), axis=1)\n",
    "    data_columns = data.columns\n",
    "    data.to_csv(\n",
    "        out_path + \"group_by_target_baseline-{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    target_count = {}\n",
    "    for EU in data_columns:\n",
    "        target = EU.split(\"-\")[0]\n",
    "        if target not in target_count:\n",
    "            target_count[target] = 0\n",
    "        target_count[target] += 1\n",
    "    # use the inverse of the target_count as the weight\n",
    "    target_weight = {key: 1/value for key, value in target_count.items()}\n",
    "    # assign EU_weight based on the target_weight\n",
    "    EU_weight = {EU: target_weight[EU.split(\"-\")[0]]\n",
    "                 for EU in data_columns}\n",
    "\n",
    "    data[\"sum\"] = data.sum(axis=1)\n",
    "    data = data.sort_values(by=\"sum\", ascending=False)\n",
    "    data.to_csv(sum_path\n",
    "                + \"sum_unweighted_baseline_{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    data.drop(columns=[\"sum\"], inplace=True)\n",
    "    # assign the weight to the data\n",
    "    data = data * pd.Series(EU_weight)\n",
    "    data[\"sum\"] = data.sum(axis=1)\n",
    "    data = data.sort_values(by=\"sum\", ascending=False)\n",
    "    data.to_csv(sum_path\n",
    "                + \"sum_baseline_{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    # get the lowest value in the data_raw, and impute nan with the lowest value\n",
    "    # data_raw = data_raw.fillna(data_raw.min().min())\n",
    "    data_raw = data_raw.reindex(sorted(data_raw.columns), axis=1)\n",
    "    data_raw[\"sum\"] = data_raw.sum(axis=1)\n",
    "    data_raw = data_raw.sort_values(by=\"sum\", ascending=False)\n",
    "    data_raw.to_csv(sum_path\n",
    "                    + \"sum_raw_unweighted_baseline_{}-{}-{}.csv\".format(feature, model, mode))\n",
    "\n",
    "    data_raw.drop(columns=[\"sum\"], inplace=True)\n",
    "    data_raw_weighted = data_raw * pd.Series(EU_weight)\n",
    "    data_raw_weighted[\"sum\"] = data_raw_weighted.sum(axis=1)\n",
    "    data_raw_weighted = data_raw_weighted.sort_values(\n",
    "        by=\"sum\", ascending=False)\n",
    "    data_raw_weighted.to_csv(sum_path\n",
    "                             + \"sum_raw_baseline_{}-{}-{}.csv\".format(feature, model, mode))\n",
    "    return data, data_raw\n",
    "\n",
    "\n",
    "data, data_raw = get_group_by_target(\n",
    "    csv_list, csv_path, feature, model, mode)\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "# print the nan rate of the data in columns and rows\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# remove rows with more than 50% nan values\n",
    "data_raw = data_raw[data_raw.isna().sum(axis=1) < 0.5 * data_raw.shape[1]]\n",
    "print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# for each row, get its non-nan values in intersection with \"TS145\", and calculate the sum of the non-nan values\n",
    "# then divide the sum of the non-nan values by the sum of \"TS145\" to get the normalized sum\n",
    "# then plot the normalized sum\n",
    "data_raw = data_raw.T\n",
    "\n",
    "groups = data_raw.columns\n",
    "baseline_group = pd.DataFrame(data_raw[\"TS145\"])\n",
    "baseline_dict = {}\n",
    "for group in groups:\n",
    "    data_raw_group = pd.DataFrame(data_raw[group])\n",
    "    # get the intersection of non nan values in data_raw_group and baseline_group\n",
    "    non_nan_baseline = baseline_group[pd.notna(baseline_group[\"TS145\"])].index\n",
    "    non_nan_group = data_raw_group[pd.notna(data_raw_group[group])].index\n",
    "    # 计算两者的交集\n",
    "    intersection_index = non_nan_baseline.intersection(non_nan_group)\n",
    "    # 根据 intersection_index 获取对应的值\n",
    "    baseline_values = baseline_group.loc[intersection_index, \"TS145\"]\n",
    "    group_values = data_raw_group.loc[intersection_index, group]\n",
    "\n",
    "    # sum them up\n",
    "    sum_baseline = baseline_values.sum()\n",
    "    sum_group = group_values.sum()\n",
    "    ratio = sum_group / sum_baseline\n",
    "    baseline_dict[group] = ratio\n",
    "    # # 输出结果或进行进一步操作\n",
    "    # print(\n",
    "    #     f\"Group {group} has {len(intersection_index)} intersecting non-NaN values.\")\n",
    "    # print(\"Baseline values:\", baseline_values.values)\n",
    "    # print(f\"{group} values:\", group_values.values)\n",
    "\n",
    "# sort the baseline_dict by its values\n",
    "baseline_dict = dict(sorted(baseline_dict.items(),\n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "groups = [key[2:] for key in baseline_dict.keys()]\n",
    "values = list(baseline_dict.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "highlight_group = \"145\"\n",
    "bar_colors = ['C0' if group != highlight_group else 'C1' for group in groups]\n",
    "plt.bar(groups, values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=20, ha='right')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\n",
    "    \"sum of {} for {} model in {} targets, compared with baseline (group {}) in CASP16\".format(feature, model, mode, highlight_group), fontsize=20, pad=20)\n",
    "plt.ylabel(\"sum of {}\".format(feature), fontsize=20)\n",
    "plt.axhline(y=1, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "for group, value in zip(groups, values):\n",
    "    if group == highlight_group:\n",
    "        plt.text(group, value + 0.03, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=20, color='C1')\n",
    "# first 5 groups\n",
    "first_5_groups = groups[:5]\n",
    "first_5_values = values[:5]\n",
    "for group, value in zip(first_5_groups, first_5_values):\n",
    "    plt.text(group, value + 0.01, str(value.round(2)),\n",
    "             ha='center', fontsize=10, color='C0')\n",
    "plt.savefig(\n",
    "    png_path + \"sum_intersect_{}-{}-{}_with_colabfold_baseline.png\".format(feature, model, mode), dpi=300)\n",
    "###############\n",
    "\n",
    "# breakpoint()\n",
    "# for feature in features:\n",
    "#     get_group_by_target(csv_list, csv_path, feature, model, mode)\n",
    "#     print(\"Finished processing {}\".format(feature))\n",
    "data_sum = data[\"sum\"]\n",
    "# plot the data_sum\n",
    "# remove the first 2 char in the index\n",
    "data_sum.index = data_sum.index.str[2:]\n",
    "plt.figure(figsize=(30, 15))\n",
    "highlight_index = \"145\"\n",
    "bar_colors = ['C0' if index !=\n",
    "              highlight_index else 'C1' for index in data_sum.index]\n",
    "\n",
    "plt.bar(data_sum.index, data_sum.values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=15, ha='right')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\n",
    "    \"sum z-score of {} for {} model in {} targets in CASP16\".format(feature, model, mode), fontsize=24)\n",
    "plt.ylabel(f\"sum {feature} z-score\", fontsize=20)\n",
    "plt.axhline(y=0, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "\n",
    "for index, value in zip(data_sum.index, data_sum.values):\n",
    "    if index == highlight_index:\n",
    "        plt.text(index, value - 5, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=15, color='C1')\n",
    "plt.savefig(\n",
    "    png_path + \"sum_z-score_{}-{}-{}.png\".format(feature, model, mode), dpi=300)\n",
    "# plt.axvline(x=\"145\", color='r')\n",
    "\n",
    "sys.exit(0)\n",
    "\n",
    "###############\n",
    "# divide by the sum of 145\n",
    "data_raw_sum = data_raw[\"sum\"]\n",
    "data_raw_sum.index = data_raw_sum.index.str[2:]\n",
    "# want to normalize the data_raw_sum by the sum of group 145\n",
    "data_raw_sum = data_raw_sum / data_raw_sum[\"145\"]\n",
    "plt.figure(figsize=(30, 15))\n",
    "highlight_index = \"145\"\n",
    "bar_colors = ['C0' if index !=\n",
    "              highlight_index else 'C1' for index in data_raw_sum.index]\n",
    "plt.bar(data_raw_sum.index, data_raw_sum.values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=12, ha='right')\n",
    "plt.title(\n",
    "    \"sum of {} for {} model in {} targets, compared with baseline (group {})\".format(feature, model, mode, highlight_index))\n",
    "plt.ylabel(\"sum of {}\".format(feature))\n",
    "plt.axhline(y=1, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "for index, value in zip(data_raw_sum.index, data_raw_sum.values):\n",
    "    if index == highlight_index:\n",
    "        plt.text(index, value + 2.0, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=10, color='C1')\n",
    "plt.savefig(\n",
    "    png_path + \"sum_{}-{}-{}_compared_with_baseline.png\".format(feature, model, mode), dpi=300)\n",
    "\n",
    "# breakpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./group_by_target_EU/GDT_HA-best-all-raw.csv\"\n",
    "GDT_HA_data = pd.read_csv(file, index_col=0)\n",
    "GDT_HA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDT_HA_data = GDT_HA_data[GDT_HA_data.isna().sum(axis=1) < 0.5 * GDT_HA_data.shape[1]]\n",
    "# print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "# print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# for each row, get its non-nan values in intersection with \"TS145\", and calculate the sum of the non-nan values\n",
    "# then divide the sum of the non-nan values by the sum of \"TS145\" to get the normalized sum\n",
    "# then plot the normalized sum\n",
    "GDT_HA_data = GDT_HA_data.T\n",
    "\n",
    "groups = GDT_HA_data.columns\n",
    "baseline_group = pd.DataFrame(GDT_HA_data[\"TS145\"])\n",
    "baseline_dict = {}\n",
    "for group in groups:\n",
    "    data_raw_group = pd.DataFrame(GDT_HA_data[group])\n",
    "    # get the intersection of non nan values in data_raw_group and baseline_group\n",
    "    non_nan_baseline = baseline_group[pd.notna(baseline_group[\"TS145\"])].index\n",
    "    non_nan_group = data_raw_group[pd.notna(data_raw_group[group])].index\n",
    "    # 计算两者的交集\n",
    "    intersection_index = non_nan_baseline.intersection(non_nan_group)\n",
    "    # 根据 intersection_index 获取对应的值\n",
    "    baseline_values = baseline_group.loc[intersection_index, \"TS145\"]\n",
    "    group_values = data_raw_group.loc[intersection_index, group]\n",
    "\n",
    "    # sum them up\n",
    "    sum_baseline = baseline_values.sum()\n",
    "    sum_group = group_values.sum()\n",
    "    ratio = sum_group / sum_baseline\n",
    "    baseline_dict[group] = ratio\n",
    "    # # 输出结果或进行进一步操作\n",
    "    # print(\n",
    "    #     f\"Group {group} has {len(intersection_index)} intersecting non-NaN values.\")\n",
    "    # print(\"Baseline values:\", baseline_values.values)\n",
    "    # print(f\"{group} values:\", group_values.valu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the baseline_dict by its values\n",
    "baseline_dict = dict(sorted(baseline_dict.items(),\n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "groups = [key[2:] for key in baseline_dict.keys()]\n",
    "values = list(baseline_dict.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 10), dpi=300)\n",
    "highlight_group = \"145\"\n",
    "bar_colors = ['C0' if group != highlight_group else 'C1' for group in groups]\n",
    "plt.bar(groups, values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=20, ha='right')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\n",
    "    \"sum of {} for {} model in {} targets, compared with baseline (group {}) in CASP16\".format(feature, model, mode, highlight_group), fontsize=20, pad=20)\n",
    "plt.ylabel(\"sum of {}\".format(feature), fontsize=20)\n",
    "plt.axhline(y=1, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "for group, value in zip(groups, values):\n",
    "    if group == highlight_group:\n",
    "        plt.text(group, value + 0.03, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=20, color='C1')\n",
    "# first 5 groups\n",
    "first_5_groups = groups[:5]\n",
    "first_5_values = values[:5]\n",
    "for group, value in zip(first_5_groups, first_5_values):\n",
    "    plt.text(group, value + 0.01, str(value.round(2)),\n",
    "             ha='center', fontsize=10, color='C0')\n",
    "plt.savefig(\n",
    "    png_path + \"sum_intersect_{}-{}-{}_with_colabfold_baseline.png\".format(feature, model, mode), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./group_by_target_CASP15/GDT_HA-best-all-raw.csv\"\n",
    "GDT_HA_data = pd.read_csv(file, index_col=0)\n",
    "GDT_HA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDT_HA_data = GDT_HA_data[GDT_HA_data.isna().sum(axis=1) < 0.5 * GDT_HA_data.shape[1]]\n",
    "# print(data_raw.isna().sum(axis=0)/data_raw.shape[0])\n",
    "# print(data_raw.isna().sum(axis=1)/data_raw.shape[1])\n",
    "# for each row, get its non-nan values in intersection with \"TS145\", and calculate the sum of the non-nan values\n",
    "# then divide the sum of the non-nan values by the sum of \"TS145\" to get the normalized sum\n",
    "# then plot the normalized sum\n",
    "GDT_HA_data = GDT_HA_data.T\n",
    "\n",
    "groups = GDT_HA_data.columns\n",
    "baseline_group = pd.DataFrame(GDT_HA_data[\"TS446\"])\n",
    "baseline_dict = {}\n",
    "for group in groups:\n",
    "    data_raw_group = pd.DataFrame(GDT_HA_data[group])\n",
    "    # get the intersection of non nan values in data_raw_group and baseline_group\n",
    "    non_nan_baseline = baseline_group[pd.notna(baseline_group[\"TS446\"])].index\n",
    "    non_nan_group = data_raw_group[pd.notna(data_raw_group[group])].index\n",
    "    # 计算两者的交集\n",
    "    intersection_index = non_nan_baseline.intersection(non_nan_group)\n",
    "    # 根据 intersection_index 获取对应的值\n",
    "    baseline_values = baseline_group.loc[intersection_index, \"TS446\"]\n",
    "    group_values = data_raw_group.loc[intersection_index, group]\n",
    "\n",
    "    # sum them up\n",
    "    sum_baseline = baseline_values.sum()\n",
    "    sum_group = group_values.sum()\n",
    "    ratio = sum_group / sum_baseline\n",
    "    baseline_dict[group] = ratio\n",
    "    # # 输出结果或进行进一步操作\n",
    "    # print(\n",
    "    #     f\"Group {group} has {len(intersection_index)} intersecting non-NaN values.\")\n",
    "    # print(\"Baseline values:\", baseline_values.values)\n",
    "    # print(f\"{group} values:\", group_values.valu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the baseline_dict by its values\n",
    "baseline_dict = dict(sorted(baseline_dict.items(),\n",
    "                     key=lambda x: x[1], reverse=True))\n",
    "groups = [key[2:] for key in baseline_dict.keys()]\n",
    "values = list(baseline_dict.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 10), dpi=300)\n",
    "highlight_group = \"446\"\n",
    "bar_colors = ['C0' if group != highlight_group else 'C1' for group in groups]\n",
    "plt.bar(groups, values, color=bar_colors)\n",
    "plt.xticks(rotation=45, fontsize=16, ha='right')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(\n",
    "    \"sum of {} for {} model in {} targets, compared with baseline (group {}) in CASP15\".format(feature, model, mode, highlight_group), fontsize=20, pad=20)\n",
    "plt.ylabel(\"sum of {}\".format(feature), fontsize=20)\n",
    "plt.axhline(y=1, color='k')\n",
    "# there is one group 145, we need to write something on top of its bar\n",
    "for group, value in zip(groups, values):\n",
    "    if group == highlight_group:\n",
    "        plt.text(group, value + 0.03, str(\"ColabFold\"),\n",
    "                 ha='center', fontsize=20, color='C1')\n",
    "# first 5 groups\n",
    "first_5_groups = groups[:5]\n",
    "first_5_values = values[:5]\n",
    "for group, value in zip(first_5_groups, first_5_values):\n",
    "    plt.text(group, value + 0.01, str(value.round(2)),\n",
    "             ha='center', fontsize=10, color='C0')\n",
    "plt.savefig(\n",
    "    png_path + \"sum_intersect_{}-{}-{}_with_colabfold_baseline.png\".format(feature, model, mode), dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
