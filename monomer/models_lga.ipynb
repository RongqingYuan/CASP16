{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, TwoSlopeNorm\n",
    "from scipy import stats\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "os.chdir('/home2/s439906/project/CASP16/monomer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1208s2-D1.txt has multiple best models: {'241': 'T1208s2TS241_5-D1', '304': 'T1208s2TS304_5-D1'}\n",
      "T1218-D1.txt has multiple best models: {'079': 'T1218TS079_2-D1', '293': 'T1218TS293_2-D1'}\n",
      "T1218-D2.txt has multiple best models: {'301': 'T1218TS301_3-D2', '284': 'T1218TS284_3-D2', '075': 'T1218TS075_3-D2', '122': 'T1218TS122_5-D2'}\n",
      "T1218-D3.txt has multiple best models: {'015': 'T1218TS015_3-D3', '059': 'T1218TS059_1-D3'}\n",
      "T1227s1-D1.txt has multiple best models: {'122': 'T1227s1TS122_1-D1', '284': 'T1227s1TS284_2-D1'}\n",
      "T1228-D2.txt has multiple best models: {'051': 'T1228v1TS051_5-D2', '375': 'T1228v1TS375_1-D2', '345': 'T1228v1TS345_5-D2'}\n",
      "T1231-D1.txt has multiple best models: {'264': 'T1231TS264_1-D1', '148': 'T1231TS148_1-D1', '312': 'T1231TS312_1-D1'}\n",
      "T1234-D1.txt has multiple best models: {'456': 'T1234TS456_4-D1', '022': 'T1234TS022_5-D1'}\n",
      "T1239-D2.txt has multiple best models: {'241': 'T1239v1TS241_3-D2', '110': 'T1239v1TS110_2-D2'}\n",
      "T1239-D3.txt has multiple best models: {'235': 'T1239v1TS235_2-D3', '028': 'T1239v1TS028_4-D3'}\n",
      "T1240-D1.txt has multiple best models: {'475': 'T1240TS475_4-D1', '284': 'T1240TS284_4-D1', '075': 'T1240TS075_4-D1', '122': 'T1240TS122_4-D1', '301': 'T1240TS301_4-D1', '198': 'T1240TS198_3-D1'}\n",
      "T1243-D1.txt has multiple best models: {'145': 'T1243TS145_5-D1', '298': 'T1243TS298_5-D1', '017': 'T1243TS017_3-D1', '023': 'T1243TS023_5-D1'}\n",
      "T1245s2-D1.txt has multiple best models: {'147': 'T1245s2TS147_2-D1', '462': 'T1245s2TS462_1-D1', '019': 'T1245s2TS019_2-D1', '148': 'T1245s2TS148_5-D1'}\n",
      "T1259-D1.txt has multiple best models: {'019': 'T1259TS019_3-D1', '218': 'T1259TS218_1-D1'}\n",
      "T1267s1-D1.txt has multiple best models: {'274': 'T1267s1TS274_4-D1', '494': 'T1267s1TS494_2-D1'}\n",
      "T1267s1-D2.txt has multiple best models: {'475': 'T1267s1TS475_2-D2', '284': 'T1267s1TS284_2-D2', '122': 'T1267s1TS122_5-D2', '301': 'T1267s1TS301_2-D2', '075': 'T1267s1TS075_2-D2'}\n",
      "T1270-D2.txt has multiple best models: {'293': 'T1270TS293_2-D2', '079': 'T1270TS079_2-D2', '014': 'T1270TS014_3-D2'}\n",
      "T1271s2-D1.txt has multiple best models: {'375': 'T1271s2TS375_5-D1', '301': 'T1271s2TS301_1-D1', '475': 'T1271s2TS475_1-D1', '284': 'T1271s2TS284_2-D1', '075': 'T1271s2TS075_1-D1'}\n",
      "T1271s5-D1.txt has multiple best models: {'015': 'T1271s5TS015_2-D1', '462': 'T1271s5TS462_3-D1', '031': 'T1271s5TS031_4-D1'}\n",
      "T1271s8-D2.txt has multiple best models: {'456': 'T1271s8TS456_3-D2', '022': 'T1271s8TS022_3-D2', '052': 'T1271s8TS052_3-D2'}\n",
      "T1272s6-D1.txt has multiple best models: {'322': 'T1272s6TS322_1-D1', '148': 'T1272s6TS148_5-D1'}\n",
      "T1274-D1.txt has multiple best models: {'304': 'T1274TS304_5-D1', '208': 'T1274TS208_1-D1', '312': 'T1274TS312_5-D1', '148': 'T1274TS148_5-D1', '375': 'T1274TS375_5-D1'}\n",
      "T1279-D1.txt has multiple best models: {'241': 'T1279TS241_1-D1', '051': 'T1279TS051_1-D1', '345': 'T1279TS345_1-D1'}\n",
      "T1292-D1.txt has multiple best models: {'331': 'T1292TS331_5-D1', '345': 'T1292TS345_5-D1', '051': 'T1292TS051_1-D1', '319': 'T1292TS319_1-D1'}\n",
      "T1294-D1.txt has multiple best models: {'051': 'T1294v1TS051_1-D1', '331': 'T1294v1TS331_2-D1', '319': 'T1294v1TS319_1-D1', '425': 'T1294v1TS425_3-D1'}\n",
      "T1295-D1.txt has multiple best models: {'269': 'T1295TS269_4-D1', '298': 'T1295TS298_5-D1'}\n",
      "T1295-D3.txt has multiple best models: {'235': 'T1295TS235_4-D3', '388': 'T1295TS388_2-D3'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'T1201-D1.txt': ['T1201TS380_4-D1', 84.3],\n",
       "  'T1206-D1.txt': ['T1206TS331_5-D1', 94.67],\n",
       "  'T1207-D1.txt': ['T1207TS465_2-D1', 71.92],\n",
       "  'T1208s1-D1.txt': ['T1208s1TS456_5-D1', 85.98],\n",
       "  'T1208s2-D1.txt': ['T1208s2TS304_5-D1', 93.63],\n",
       "  'T1210-D1.txt': ['T1210TS369_3-D1', 51.66],\n",
       "  'T1212-D1.txt': ['T1212TS052_1-D1', 74.14],\n",
       "  'T1218-D1.txt': ['T1218TS079_2-D1', 74.96],\n",
       "  'T1218-D2.txt': ['T1218TS122_5-D2', 75.07],\n",
       "  'T1218-D3.txt': ['T1218TS059_1-D3', 88.81],\n",
       "  'T1220s1-D1.txt': ['T1220s1TS290_5-D1', 49.66],\n",
       "  'T1226-D1.txt': ['T1226TS294_1-D1', 52.66],\n",
       "  'T1227s1-D1.txt': ['T1227s1TS122_1-D1', 81.12],\n",
       "  'T1228-D1.txt': ['T1228v1TS314_2-D1', 82.01],\n",
       "  'T1228-D2.txt': ['T1228v1TS375_1-D2', 86.67],\n",
       "  'T1228-D3.txt': ['T1228v1TS163_3-D3', 55.59],\n",
       "  'T1228-D4.txt': ['T1228v1TS120_1-D4', 59.85],\n",
       "  'T1230s1-D1.txt': ['T1230s1TS287_5-D1', 77.17],\n",
       "  'T1231-D1.txt': ['T1231TS264_1-D1', 90.32],\n",
       "  'T1234-D1.txt': ['T1234TS022_5-D1', 91.84],\n",
       "  'T1235-D1.txt': ['T1235TS425_1-D1', 95.75],\n",
       "  'T1237-D1.txt': ['T1237TS287_1-D1', 81.8],\n",
       "  'T1239-D1.txt': ['T1239v1TS110_5-D1', 80.33],\n",
       "  'T1239-D2.txt': ['T1239v1TS241_3-D2', 88.33],\n",
       "  'T1239-D3.txt': ['T1239v1TS235_2-D3', 72.56],\n",
       "  'T1239-D4.txt': ['T1239v1TS351_3-D4', 64.39],\n",
       "  'T1240-D1.txt': ['T1240TS284_4-D1', 94.23],\n",
       "  'T1240-D2.txt': ['T1240TS262_5-D2', 82.72],\n",
       "  'T1243-D1.txt': ['T1243TS023_5-D1', 75.09],\n",
       "  'T1244s1-D1.txt': ['T1244s1TS290_1-D1', 95.59],\n",
       "  'T1245s1-D1.txt': ['T1245s1TS031_1-D1', 88.23],\n",
       "  'T1245s2-D1.txt': ['T1245s2TS462_1-D1', 83.83],\n",
       "  'T1246-D1.txt': ['T1246TS022_1-D1', 88.54],\n",
       "  'T1249v1-D1.txt': ['T1249v1TS397_1-D1', 78.11],\n",
       "  'T1257-D1.txt': ['T1257TS019_1-D1', 70.8],\n",
       "  'T1259-D1.txt': ['T1259TS218_1-D1', 92.89],\n",
       "  'T1266-D1.txt': ['T1266TS163_1-D1', 74.92],\n",
       "  'T1267s1-D1.txt': ['T1267s1TS274_4-D1', 55.2],\n",
       "  'T1267s1-D2.txt': ['T1267s1TS301_2-D2', 82.81],\n",
       "  'T1267s2-D1.txt': ['T1267s2TS290_1-D1', 76.89],\n",
       "  'T1269-D1.txt': ['T1269TS031_2-D1', 72.31],\n",
       "  'T1269-D2.txt': ['T1269TS345_2-D2', 71.23],\n",
       "  'T1269-D3.txt': ['T1269TS164_1-D3', 70.81],\n",
       "  'T1270-D1.txt': ['T1270TS369_1-D1', 85.24],\n",
       "  'T1270-D2.txt': ['T1270TS014_3-D2', 69.88],\n",
       "  'T1271s1-D1.txt': ['T1271s1TS208_3-D1', 56.1],\n",
       "  'T1271s2-D1.txt': ['T1271s2TS475_1-D1', 71.65],\n",
       "  'T1271s3-D1.txt': ['T1271s3TS091_5-D1', 77.41],\n",
       "  'T1271s4-D1.txt': ['T1271s4TS015_2-D1', 72.68],\n",
       "  'T1271s5-D1.txt': ['T1271s5TS031_4-D1', 74.03],\n",
       "  'T1271s5-D2.txt': ['T1271s5TS311_5-D2', 75.47],\n",
       "  'T1271s6-D1.txt': ['T1271s6TS267_3-D1', 89.25],\n",
       "  'T1271s7-D1.txt': ['T1271s7TS031_2-D1', 81.71],\n",
       "  'T1271s8-D1.txt': ['T1271s8TS208_4-D1', 74.25],\n",
       "  'T1271s8-D2.txt': ['T1271s8TS022_3-D2', 72.88],\n",
       "  'T1272s2-D1.txt': ['T1272s2TS015_1-D1', 80.51],\n",
       "  'T1272s6-D1.txt': ['T1272s6TS322_1-D1', 82.7],\n",
       "  'T1272s8-D1.txt': ['T1272s8TS031_5-D1', 82.39],\n",
       "  'T1272s9-D1.txt': ['T1272s9TS031_5-D1', 75.14],\n",
       "  'T1274-D1.txt': ['T1274TS208_1-D1', 98.08],\n",
       "  'T1276-D1.txt': ['T1276TS287_1-D1', 88.27],\n",
       "  'T1278-D1.txt': ['T1278TS298_1-D1', 97.62],\n",
       "  'T1279-D1.txt': ['T1279TS241_1-D1', 96.88],\n",
       "  'T1279-D2.txt': ['T1279TS294_2-D2', 78.17],\n",
       "  'T1280-D1.txt': ['T1280TS294_3-D1', 93.81],\n",
       "  'T1284-D1.txt': ['T1284TS015_1-D1', 82.56],\n",
       "  'T1292-D1.txt': ['T1292TS051_1-D1', 98.06],\n",
       "  'T1294-D1.txt': ['T1294v1TS051_1-D1', 93.48],\n",
       "  'T1295-D1.txt': ['T1295TS269_4-D1', 80.78],\n",
       "  'T1295-D2.txt': ['T1295TS269_4-D2', 89.49],\n",
       "  'T1295-D3.txt': ['T1295TS388_2-D3', 76.12],\n",
       "  'T1298-D1.txt': ['T1298TS148_3-D1', 78.57],\n",
       "  'T1298-D2.txt': ['T1298TS489_5-D2', 80.09],\n",
       "  'T1299-D1.txt': ['T1299TS015_5-D1', 85.72]},\n",
       " {'T1201-D1.txt': ['T1201TS191_1-D1', 79.42],\n",
       "  'T1206-D1.txt': ['T1206TS122_1-D1', 94.35],\n",
       "  'T1207-D1.txt': ['T1207TS465_1-D1', 70.7],\n",
       "  'T1208s1-D1.txt': ['T1208s1TS274_1-D1', 85.82],\n",
       "  'T1208s2-D1.txt': ['T1208s2TS304_1-D1', 93.63],\n",
       "  'T1210-D1.txt': ['T1210TS051_1-D1', 49.8],\n",
       "  'T1212-D1.txt': ['T1212TS052_1-D1', 74.14],\n",
       "  'T1218-D1.txt': ['T1218TS015_1-D1', 77.56],\n",
       "  'T1218-D2.txt': ['T1218TS208_1-D2', 78.6],\n",
       "  'T1218-D3.txt': ['T1218TS059_1-D3', 88.81],\n",
       "  'T1220s1-D1.txt': ['T1220s1TS196_1-D1', 47.84],\n",
       "  'T1226-D1.txt': ['T1226TS294_1-D1', 52.66],\n",
       "  'T1227s1-D1.txt': ['T1227s1TS122_1-D1', 81.12],\n",
       "  'T1228-D1.txt': ['T1228v1TS388_1-D1', 79.11],\n",
       "  'T1228-D2.txt': ['T1228v1TS375_1-D2', 86.67],\n",
       "  'T1228-D3.txt': ['T1228v1TS465_1-D3', 55.44],\n",
       "  'T1228-D4.txt': ['T1228v1TS120_1-D4', 59.85],\n",
       "  'T1230s1-D1.txt': ['T1230s1TS208_1-D1', 75.5],\n",
       "  'T1231-D1.txt': ['T1231TS264_1-D1', 90.32],\n",
       "  'T1234-D1.txt': ['T1234TS091_1-D1', 92.64],\n",
       "  'T1235-D1.txt': ['T1235TS425_1-D1', 95.75],\n",
       "  'T1237-D1.txt': ['T1237TS287_1-D1', 81.8],\n",
       "  'T1239-D1.txt': ['T1239v1TS204_1-D1', 78.96],\n",
       "  'T1239-D2.txt': ['T1239v1TS019_1-D2', 89.81],\n",
       "  'T1239-D3.txt': ['T1239v1TS235_1-D3', 72.56],\n",
       "  'T1239-D4.txt': ['T1239v1TS361_1-D4', 62.5],\n",
       "  'T1240-D1.txt': ['T1240TS051_1-D1', 94.95],\n",
       "  'T1240-D2.txt': ['T1240TS262_1-D2', 82.55],\n",
       "  'T1243-D1.txt': ['T1243TS388_1-D1', 76.41],\n",
       "  'T1244s1-D1.txt': ['T1244s1TS290_1-D1', 95.59],\n",
       "  'T1245s1-D1.txt': ['T1245s1TS031_1-D1', 88.23],\n",
       "  'T1245s2-D1.txt': ['T1245s2TS462_1-D1', 83.83],\n",
       "  'T1246-D1.txt': ['T1246TS022_1-D1', 88.54],\n",
       "  'T1249v1-D1.txt': ['T1249v1TS397_1-D1', 78.11],\n",
       "  'T1257-D1.txt': ['T1257TS019_1-D1', 70.8],\n",
       "  'T1259-D1.txt': ['T1259TS218_1-D1', 92.89],\n",
       "  'T1266-D1.txt': ['T1266TS163_1-D1', 74.92],\n",
       "  'T1267s1-D1.txt': ['T1267s1TS331_1-D1', 72.08],\n",
       "  'T1267s1-D2.txt': ['T1267s1TS301_1-D2', 82.81],\n",
       "  'T1267s2-D1.txt': ['T1267s2TS290_1-D1', 76.89],\n",
       "  'T1269-D1.txt': ['T1269TS031_1-D1', 71.88],\n",
       "  'T1269-D2.txt': ['T1269TS052_1-D2', 71.11],\n",
       "  'T1269-D3.txt': ['T1269TS164_1-D3', 70.81],\n",
       "  'T1270-D1.txt': ['T1270TS369_1-D1', 85.24],\n",
       "  'T1270-D2.txt': ['T1270TS274_1-D2', 72.38],\n",
       "  'T1271s1-D1.txt': ['T1271s1TS208_1-D1', 55.8],\n",
       "  'T1271s2-D1.txt': ['T1271s2TS475_1-D1', 71.65],\n",
       "  'T1271s3-D1.txt': ['T1271s3TS272_1-D1', 76.93],\n",
       "  'T1271s4-D1.txt': ['T1271s4TS301_1-D1', 68.79],\n",
       "  'T1271s5-D1.txt': ['T1271s5TS031_1-D1', 74.03],\n",
       "  'T1271s5-D2.txt': ['T1271s5TS148_1-D2', 74.3],\n",
       "  'T1271s6-D1.txt': ['T1271s6TS051_1-D1', 87.63],\n",
       "  'T1271s7-D1.txt': ['T1271s7TS022_1-D1', 80.33],\n",
       "  'T1271s8-D1.txt': ['T1271s8TS091_1-D1', 71.21],\n",
       "  'T1271s8-D2.txt': ['T1271s8TS022_1-D2', 72.88],\n",
       "  'T1272s2-D1.txt': ['T1272s2TS015_1-D1', 80.51],\n",
       "  'T1272s6-D1.txt': ['T1272s6TS322_1-D1', 82.7],\n",
       "  'T1272s8-D1.txt': ['T1272s8TS031_1-D1', 82.3],\n",
       "  'T1272s9-D1.txt': ['T1272s9TS031_1-D1', 74.72],\n",
       "  'T1274-D1.txt': ['T1274TS208_1-D1', 98.08],\n",
       "  'T1276-D1.txt': ['T1276TS287_1-D1', 88.27],\n",
       "  'T1278-D1.txt': ['T1278TS298_1-D1', 97.62],\n",
       "  'T1279-D1.txt': ['T1279TS241_1-D1', 96.88],\n",
       "  'T1279-D2.txt': ['T1279TS294_1-D2', 77.5],\n",
       "  'T1280-D1.txt': ['T1280TS294_1-D1', 93.06],\n",
       "  'T1284-D1.txt': ['T1284TS015_1-D1', 82.56],\n",
       "  'T1292-D1.txt': ['T1292TS051_1-D1', 98.06],\n",
       "  'T1294-D1.txt': ['T1294v1TS051_1-D1', 93.48],\n",
       "  'T1295-D1.txt': ['T1295TS122_1-D1', 81.56],\n",
       "  'T1295-D2.txt': ['T1295TS269_1-D2', 88.77],\n",
       "  'T1295-D3.txt': ['T1295TS388_1-D3', 76.12],\n",
       "  'T1298-D1.txt': ['T1298TS022_1-D1', 77.73],\n",
       "  'T1298-D2.txt': ['T1298TS489_1-D2', 77.83],\n",
       "  'T1299-D1.txt': ['T1299TS293_1-D1', 84.23]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_txt_dir = \"/home2/s439906/data/CASP16/web_Feb_7/\"\n",
    "data_files = [file for file in os.listdir(data_txt_dir) if file.endswith('.txt') and file.startswith('T1')]\n",
    "data_files.sort()\n",
    "\n",
    "target_dir = '/data/data1/conglab/qcong/CASP16/stage1_monomer_inputs/targets/'\n",
    "model_dir = '/home2/s439906/data/CASP16/CASP16_tarball/predictions_trimmed_to_domains/'\n",
    "\n",
    "lga_target_pdb_dir = \"./model_lga_job/target_pdb_files/\"\n",
    "lga_best_model_pdb_dir = './model_pdb_files_best/'\n",
    "lga_first_model_pdb_dir = './model_pdb_files_first/'\n",
    "lga_first_of_best_model_pdb_dir = './model_pdb_files_first_of_best/'\n",
    "\n",
    "best_models = {}\n",
    "first_models = {}\n",
    "for file in data_files:\n",
    "    best_models_dict = {}\n",
    "    best_model = None\n",
    "    best_GDT_HA = 0\n",
    "    with open(os.path.join(data_txt_dir, file), 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                if words[0] != \"#\":\n",
    "                    file_name = words[1]\n",
    "                    GDT_TS = words[4]\n",
    "                    GDT_HA = words[9]\n",
    "                    group = file_name.split('TS')[1][:3]\n",
    "                    model_number = file_name.split('_')[1][0]\n",
    "                    if float(GDT_HA) > best_GDT_HA and 'v2' not in file_name:\n",
    "                        best_GDT_HA = float(GDT_HA)\n",
    "                        best_model = file_name\n",
    "                        best_models_dict ={}\n",
    "                        best_models_dict[group] = best_model\n",
    "                    elif float(GDT_HA) == best_GDT_HA and 'v2' not in file_name:\n",
    "                        best_model = file_name\n",
    "                        best_models_dict[group] = best_model\n",
    "    if len(best_models_dict) == 1:\n",
    "        best_models[file] = [best_model, best_GDT_HA]\n",
    "    else:\n",
    "        print(f\"{file} has multiple best models: {best_models_dict}\")\n",
    "        first_GDT_HA = 0\n",
    "        best_group = None\n",
    "        with open(os.path.join(data_txt_dir, file), 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                if len(words) > 1:\n",
    "                    if words[0] != \"#\":\n",
    "                        file_name = words[1]\n",
    "                        GDT_TS = words[4]\n",
    "                        GDT_HA = words[9]\n",
    "                        group = file_name.split('TS')[1][:3]\n",
    "                        model_number = file_name.split('_')[1][0]\n",
    "                        if group in best_models_dict and model_number == '1' and float(GDT_HA) > first_GDT_HA and 'v2' not in file_name:\n",
    "                            first_GDT_HA = float(GDT_HA)\n",
    "                            best_group = group\n",
    "        if best_group is None:\n",
    "            print(f\"{file} does not have a first model\")\n",
    "        else:\n",
    "            best_model = best_models_dict[best_group]\n",
    "            best_models[file] = [best_model, first_GDT_HA]\n",
    "for file in data_files:\n",
    "    first_model = None\n",
    "    first_GDT_HA = 0\n",
    "    with open(os.path.join(data_txt_dir, file), 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                if words[0] != \"#\":\n",
    "                    file_name = words[1]\n",
    "                    GDT_TS = words[4]\n",
    "                    GDT_HA = words[9]\n",
    "                    group = file_name.split('TS')[1][:3]\n",
    "                    model_number = file_name.split('_')[1][0]\n",
    "                    if float(GDT_HA) > first_GDT_HA and model_number == '1' and 'v2' not in file_name:\n",
    "                        first_GDT_HA = float(GDT_HA)\n",
    "                        first_model = file_name\n",
    "    first_models[file] = [first_model, first_GDT_HA]\n",
    "best_models, first_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lga_best_job_dir = \"./model_lga_job/best_job/\"\n",
    "lga_first_job_dir = \"./model_lga_job/first_job/\"\n",
    "lga_first_of_best_job_dir = \"./model_lga_job/first_of_best_job/\"\n",
    "\n",
    "if not os.path.exists(lga_best_job_dir):\n",
    "    os.makedirs(lga_best_job_dir)\n",
    "if not os.path.exists(lga_first_job_dir):\n",
    "    os.makedirs(lga_first_job_dir)\n",
    "if not os.path.exists(lga_first_of_best_job_dir):\n",
    "    os.makedirs(lga_first_of_best_job_dir)\n",
    "if not os.path.exists(os.path.join(lga_best_job_dir, 'MOL2')):    \n",
    "    os.makedirs(os.path.join(lga_best_job_dir, 'MOL2'))\n",
    "if not os.path.exists(os.path.join(lga_best_job_dir, 'TMP')):\n",
    "    os.makedirs(os.path.join(lga_best_job_dir, 'TMP'))\n",
    "if not os.path.exists(os.path.join(lga_best_job_dir, 'RESULTS')):\n",
    "    os.makedirs(os.path.join(lga_best_job_dir, 'RESULTS'))\n",
    "if not os.path.exists(os.path.join(lga_first_job_dir, 'MOL2')):    \n",
    "    os.makedirs(os.path.join(lga_first_job_dir, 'MOL2'))\n",
    "if not os.path.exists(os.path.join(lga_first_job_dir, 'TMP')):\n",
    "    os.makedirs(os.path.join(lga_first_job_dir, 'TMP'))\n",
    "if not os.path.exists(os.path.join(lga_first_job_dir, 'RESULTS')):\n",
    "    os.makedirs(os.path.join(lga_first_job_dir, 'RESULTS'))\n",
    "if not os.path.exists(os.path.join(lga_first_of_best_job_dir, 'MOL2')):    \n",
    "    os.makedirs(os.path.join(lga_first_of_best_job_dir, 'MOL2'))\n",
    "if not os.path.exists(os.path.join(lga_first_of_best_job_dir, 'TMP')):\n",
    "    os.makedirs(os.path.join(lga_first_of_best_job_dir, 'TMP'))\n",
    "if not os.path.exists(os.path.join(lga_first_of_best_job_dir, 'RESULTS')):\n",
    "    os.makedirs(os.path.join(lga_first_of_best_job_dir, 'RESULTS'))\n",
    "\n",
    "\n",
    "\n",
    "for file, (model, GDT_HA) in best_models.items():\n",
    "    chain = 'A' # Andriy did not provide the chain information in the target file\n",
    "    \n",
    "    target_name = model.split('TS')[0]\n",
    "    # if target_name.startswith('T1239v2'):\n",
    "    #     target_name = 'T1239v1' # there is no T1239v2 in the target_dir\n",
    "    domain = model.split('-')[1]\n",
    "    target_file = os.path.join(target_dir, target_name+'-'+domain+'.pdb')\n",
    "    if not os.path.exists(target_file):\n",
    "        print(f\"{target_file} does not exist\")\n",
    "    else:\n",
    "        os.system(f\"cp {target_file} {lga_target_pdb_dir}\")\n",
    "        target_lines = []\n",
    "        with open(target_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('ATOM'):\n",
    "                    if line[21] == ' ':\n",
    "                        line = line[:21] + chain + line[22:]\n",
    "                    target_lines.append(line)\n",
    "\n",
    "    target_name = model.split('TS')[0]\n",
    "    domain = model.split('-')[1]\n",
    "    best_model_file = os.path.join(model_dir, target_name+'-'+domain, model)\n",
    "    if not os.path.exists(best_model_file):\n",
    "        print(f\"{best_model_file} does not exist\")\n",
    "    else:\n",
    "        os.system(f\"cp {best_model_file} {lga_best_model_pdb_dir}\")\n",
    "        best_model_lines = []\n",
    "        with open(best_model_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('ATOM'):\n",
    "                    if line[21] == ' ':\n",
    "                        line = line[:21] + chain + line[22:]\n",
    "                    best_model_lines.append(line)\n",
    "\n",
    "    target_name = model.split('TS')[0]\n",
    "    domain = model.split('-')[1]\n",
    "    best_group = model.split('TS')[1][:3]\n",
    "    first_of_best_model = None\n",
    "    first_of_best_GDT_HA = 0\n",
    "    with open(os.path.join(data_txt_dir, file), 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                if words[0] != \"#\":\n",
    "                    file_name = words[1]\n",
    "                    GDT_TS = words[4]\n",
    "                    GDT_HA = words[9]\n",
    "                    group = file_name.split('TS')[1][:3]\n",
    "                    model_number = file_name.split('_')[1][0]\n",
    "                    if float(GDT_HA) > first_of_best_GDT_HA and group == best_group and model_number == '1' and 'v2' not in file_name:\n",
    "                        first_of_best_GDT_HA = float(GDT_HA)\n",
    "                        first_of_best_model = file_name\n",
    "    if first_of_best_model is None:\n",
    "        print(f\"{file} does not have a first of best model\")\n",
    "    else:\n",
    "        target_name = first_of_best_model.split('TS')[0] # because of the v1 v2 problem, we need to update the target name again\n",
    "        first_of_best_model_file = os.path.join(model_dir, target_name+'-'+domain, first_of_best_model)\n",
    "        if not os.path.exists(first_of_best_model_file):\n",
    "            print(f\"{first_of_best_model_file} does not exist\")\n",
    "        else:\n",
    "            os.system(f\"cp {first_of_best_model_file} {lga_first_of_best_model_pdb_dir}\")\n",
    "            first_of_best_model_lines = []\n",
    "            with open(first_of_best_model_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.startswith('ATOM'):\n",
    "                        if line[21] == ' ':\n",
    "                            line = line[:21] + chain + line[22:]\n",
    "                        first_of_best_model_lines.append(line)\n",
    "    \n",
    "    with open(os.path.join(os.path.join(lga_best_job_dir, 'MOL2'), best_model_file.split('/')[-1]), 'w') as f:\n",
    "        f.write('MOLECULE\\t'+best_model_file.split('/')[-1]+'\\n')\n",
    "        for line in best_model_lines:\n",
    "            f.write(line)\n",
    "        f.write('END\\n')\n",
    "        f.write('MOLECULE\\t'+target_file.split('/')[-1]+'\\n')\n",
    "        for line in target_lines:\n",
    "            if line[-2] != 'H':\n",
    "                f.write(line)\n",
    "        f.write('END\\n')\n",
    "    \n",
    "    with open(os.path.join(os.path.join(lga_first_of_best_job_dir, 'MOL2'), first_of_best_model_file.split('/')[-1]), 'w') as f:\n",
    "        f.write('MOLECULE\\t'+first_of_best_model_file.split('/')[-1]+'\\n')\n",
    "        for line in first_of_best_model_lines:\n",
    "            f.write(line)\n",
    "        f.write('END\\n')\n",
    "        f.write('MOLECULE\\t'+target_file.split('/')[-1]+'\\n')\n",
    "        for line in target_lines:\n",
    "            if line[-2] != 'H':\n",
    "                f.write(line)\n",
    "        f.write('END\\n')\n",
    "\n",
    "for file, (model, GDT_HA) in first_models.items():\n",
    "    chain = 'A' # Andriy did not provide the chain information in the target file\n",
    "    \n",
    "    target_name = model.split('TS')[0]\n",
    "    if target_name.startswith('T1239v2'):\n",
    "        target_name = 'T1239v1' # there is no T1239v2 in the target_dir\n",
    "    domain = model.split('-')[1]\n",
    "    target_file = os.path.join(target_dir, target_name+'-'+domain+'.pdb')\n",
    "    if not os.path.exists(target_file):\n",
    "        print(f\"{target_file} does not exist\")\n",
    "    else:\n",
    "        os.system(f\"cp {target_file} {lga_target_pdb_dir}\")\n",
    "        target_lines = []\n",
    "        with open(target_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('ATOM'):\n",
    "                    if line[21] == ' ':\n",
    "                        line = line[:21] + chain + line[22:]\n",
    "                    target_lines.append(line)\n",
    "\n",
    "    target_name = model.split('TS')[0]\n",
    "    domain = model.split('-')[1]\n",
    "    first_model_file = os.path.join(model_dir, target_name+'-'+domain, model)\n",
    "    if not os.path.exists(first_model_file):\n",
    "        print(f\"{first_model_file} does not exist\")\n",
    "    else:\n",
    "        os.system(f\"cp {first_model_file} {lga_first_model_pdb_dir}\")\n",
    "        first_model_lines = []\n",
    "        with open(first_model_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('ATOM'):\n",
    "                    if line[21] == ' ':\n",
    "                        line = line[:21] + chain + line[22:]\n",
    "                    first_model_lines.append(line)\n",
    "    \n",
    "    with open(os.path.join(os.path.join(lga_first_job_dir, 'MOL2'), first_model_file.split('/')[-1]), 'w') as f:\n",
    "        f.write('MOLECULE\\t'+first_model_file.split('/')[-1]+'\\n')\n",
    "        for line in first_model_lines:\n",
    "            f.write(line)\n",
    "        f.write('END\\n')\n",
    "        f.write('MOLECULE\\t'+target_file.split('/')[-1]+'\\n')\n",
    "        for line in target_lines:\n",
    "            if line[-2] != 'H':\n",
    "                f.write(line)\n",
    "        f.write('END\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "home_dir = \"./model_lga_job/\"\n",
    "job_dirs = ['best_job/', 'first_job/', 'first_of_best_job/']\n",
    "for job_dir in job_dirs:\n",
    "    input_dir = os.path.join(home_dir, job_dir, 'MOL2')\n",
    "    if len(os.listdir(input_dir)) !=74:\n",
    "        print(f\"{input_dir} does not have 74 files, please check\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    with open(os.path.join(home_dir, job_dir, \"lga_cmd\"), 'w') as f:\n",
    "        shebang=f\"\"\"#!/bin/bash\n",
    "#SBATCH -p 256GB\n",
    "#SBATCH -n 48\n",
    "#SBATCH -t 28-00:00:00\n",
    "#SBATCH -o {job_dir[:-1]}.log\n",
    "#SBATCH -e {job_dir[:-1]}.err\n",
    "#SBATCH -J {job_dir[:-1]}\n",
    "#SBATCH --mem=250GB\"\"\"\n",
    "        f.write(shebang)\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('ulimit -s unlimited\\n')\n",
    "        for file in os.listdir(input_dir):\n",
    "            f.write(f'/data/data1/conglab/jzhan6/software/LGA_package_src/lga -3  -ie  -o1  -sda  -d:4  -gdc_sc {file}\\n')\n",
    "        os.chdir(os.path.join(home_dir, job_dir))\n",
    "        os.system(f'sbatch lga_cmd')\n",
    "        os.chdir(current_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch ./model_lga_job/best_job/lga_cmd\n",
      "sbatch ./model_lga_job/first_job/lga_cmd\n",
      "sbatch ./model_lga_job/first_of_best_job/lga_cmd\n"
     ]
    }
   ],
   "source": [
    "home_dir = \"./model_lga_job/\"\n",
    "job_dirs = ['best_job/', 'first_job/', 'first_of_best_job/']\n",
    "for job_dir in job_dirs:\n",
    "    input_dir = os.path.join(home_dir, job_dir, 'MOL2')\n",
    "    if len(os.listdir(input_dir)) !=74:\n",
    "        print(f\"{input_dir} does not have 74 files, please check\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    with open(os.path.join(home_dir, job_dir, \"lga_cmd\"), 'w') as f:\n",
    "        shebang=f\"\"\"#!/bin/bash\n",
    "#SBATCH -p 256GB\n",
    "#SBATCH -n 48\n",
    "#SBATCH -t 28-00:00:00\n",
    "#SBATCH -o {job_dir[:-1]}.log\n",
    "#SBATCH -e {job_dir[:-1]}.err\n",
    "#SBATCH -J {job_dir[:-1]}\n",
    "#SBATCH --mem=250GB\"\"\"\n",
    "        f.write(shebang)\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('ulimit -s unlimited\\n')\n",
    "        for file in os.listdir(input_dir):\n",
    "            f.write(f'/data/data1/conglab/jzhan6/software/LGA_package_src/lga -3  -ie  -o1  -sda  -d:4  -gdc_sc {file}\\n')\n",
    "        # os.system(f'sbatch {os.path.join(home_dir, job_dir, \"lga_cmd\")}')\n",
    "        print(f'sbatch {os.path.join(home_dir, job_dir, \"lga_cmd\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, (model, GDT_HA) in first_models.items():\n",
    "    target_name = model.split('TS')[0]\n",
    "    domain = model.split('-')[1]\n",
    "    first_model_file = os.path.join(model_dir, target_name+'-'+domain, model)\n",
    "    if not os.path.exists(first_model_file):\n",
    "        print(f\"{first_model_file} does not exist\")\n",
    "    else:\n",
    "        os.system(f\"cp {best_model_file} {lga_first_model_pdb_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, (model, GDT_HA) in best_models.items():\n",
    "    target_name = model.split('TS')[0]\n",
    "    domain = model.split('-')[1]\n",
    "    best_model_file = os.path.join(model_dir, target_name+'-'+domain, model)\n",
    "    if not os.path.exists(best_model_file):\n",
    "        print(f\"{best_model_file} does not exist\")\n",
    "    else:\n",
    "        os.system(f\"cp {best_model_file} {lga_best_model_pdb_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, (model, GDT_HA) in best_models.items():\n",
    "    # print(file, model, GDT_HA)\n",
    "    target_name = model.split('TS')[0]\n",
    "    domain = model.split('-')[1]\n",
    "    group = model.split('TS')[1][:3]\n",
    "    first_of_best_model = None\n",
    "    first_of_best_GDT_HA = 0\n",
    "    with open(os.path.join(data_txt_dir, file), 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            if len(words) > 1:\n",
    "                if words[0] != \"#\":\n",
    "                    file_name = words[1]\n",
    "                    GDT_TS = words[4]\n",
    "                    GDT_HA = words[9]\n",
    "                    group = file_name.split('TS')[1][:3]\n",
    "                    model_number = file_name.split('_')[1][0]\n",
    "                    if group == group and model_number == '1':\n",
    "                        if float(GDT_HA) > first_of_best_GDT_HA:\n",
    "                            first_of_best_GDT_HA = float(GDT_HA)\n",
    "                            first_of_best_model = file_name\n",
    "    if first_of_best_model is None:\n",
    "        print(f\"{file} does not have a first of best model\")\n",
    "    else:\n",
    "        target_name = first_of_best_model.split('TS')[0] #because of the v1 v2 problem, we need to update the target name again\n",
    "        first_of_best_model_file = os.path.join(model_dir, target_name+'-'+domain, first_of_best_model)\n",
    "        if not os.path.exists(first_of_best_model_file):\n",
    "            print(f\"{first_of_best_model_file} does not exist\")\n",
    "        else:\n",
    "            os.system(f\"cp {first_of_best_model_file} {lga_first_of_best_model_pdb_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lga_best_job_dir = \"./model_lga_job/best_job/\"\n",
    "if not os.path.exists(lga_best_job_dir):\n",
    "    os.makedirs(lga_best_job_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
